<!DOCTYPE html>
<html lang="en">

<head><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<link href="https://fonts.googleapis.com/css?family=Merriweather:300|Raleway:400,700" rel="stylesheet">
<link rel="stylesheet" href="/assets/css/style.css">
<title>Week1 - 토요일</title>
<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Week1 - 토요일 | AI Tech Study</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Week1 - 토요일" />
<meta name="author" content="LKH" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="1. 개인학습 베이즈 통계 조건부 확률을 활용해 정보를 갱신하는 방법 조건부 확률 $Y$ 가 관찰되었을 때, X가 발생할 확률 $ P(X \vert Y) = {P(X,Y) \over P(Y)} $ 베이즈 정리 $P(\theta \vert D) = P(\theta) {P(D \vert \theta) \over P(D)}$ $P(\theta \vert D)$ : 사후확률(posterior) $P(\theta)$ : 사전확률(prior) $P(D \vert \theta)$ : 가능도(likelihood) $P(D)$ : Evidence 면접 복기 프로젝트 소개 프로젝트 관련 기초지식(ML/AI + CS) 이론 메트릭 Why? 2. 선택과제 복습 1번 미니배치 효과 불안정하지만 빠른 수렴 Convex 형태가 아닌 딥러닝 모델을 수렴시키기에 더 적합 2번 시간 역순으로 전파되어오는 그래디언트들의 합! $\frac{\partial \xi}{\partial W_x} = \frac{\partial \xi}{\partial y} \frac{\partial y}{\partial S_n} \frac{\partial S_n}{\partial W_x} + \frac{\partial \xi}{\partial y} \frac{\partial y}{\partial S_n} \frac{\partial S_n}{\partial S_{n-1}}\frac{\partial S_{n-1}}{\partial W_x} \cdots = \sum\limits_{k=0}^n \frac{\partial \xi}{\partial y} \frac{\partial y}{\partial S_k} \frac{\partial S_k}{\partial W_x} = \sum\limits_{k=0}^n \frac{\partial \xi}{\partial S_k} X_k $ $\frac{\partial \xi}{\partial W_{rec}} = \frac{\partial \xi}{\partial y} \frac{\partial y}{\partial S_n} \frac{\partial S_n}{\partial W_{rec}} + \frac{\partial \xi}{\partial y} \frac{\partial y}{\partial S_n} \frac{\partial S_n}{\partial S_{n-1}}\frac{\partial S_{n-1}}{\partial W_{rec}} \cdots = \sum\limits_{k=0}^n \frac{\partial \xi}{\partial y} \frac{\partial y}{\partial S_k} \frac{\partial S_k}{\partial W_{rec}} = \sum\limits_{k=1}^n \frac{\partial \xi}{\partial S_k} S_{k-1} $ 3번 용어! 표본평균 / 표본분산" />
<meta property="og:description" content="1. 개인학습 베이즈 통계 조건부 확률을 활용해 정보를 갱신하는 방법 조건부 확률 $Y$ 가 관찰되었을 때, X가 발생할 확률 $ P(X \vert Y) = {P(X,Y) \over P(Y)} $ 베이즈 정리 $P(\theta \vert D) = P(\theta) {P(D \vert \theta) \over P(D)}$ $P(\theta \vert D)$ : 사후확률(posterior) $P(\theta)$ : 사전확률(prior) $P(D \vert \theta)$ : 가능도(likelihood) $P(D)$ : Evidence 면접 복기 프로젝트 소개 프로젝트 관련 기초지식(ML/AI + CS) 이론 메트릭 Why? 2. 선택과제 복습 1번 미니배치 효과 불안정하지만 빠른 수렴 Convex 형태가 아닌 딥러닝 모델을 수렴시키기에 더 적합 2번 시간 역순으로 전파되어오는 그래디언트들의 합! $\frac{\partial \xi}{\partial W_x} = \frac{\partial \xi}{\partial y} \frac{\partial y}{\partial S_n} \frac{\partial S_n}{\partial W_x} + \frac{\partial \xi}{\partial y} \frac{\partial y}{\partial S_n} \frac{\partial S_n}{\partial S_{n-1}}\frac{\partial S_{n-1}}{\partial W_x} \cdots = \sum\limits_{k=0}^n \frac{\partial \xi}{\partial y} \frac{\partial y}{\partial S_k} \frac{\partial S_k}{\partial W_x} = \sum\limits_{k=0}^n \frac{\partial \xi}{\partial S_k} X_k $ $\frac{\partial \xi}{\partial W_{rec}} = \frac{\partial \xi}{\partial y} \frac{\partial y}{\partial S_n} \frac{\partial S_n}{\partial W_{rec}} + \frac{\partial \xi}{\partial y} \frac{\partial y}{\partial S_n} \frac{\partial S_n}{\partial S_{n-1}}\frac{\partial S_{n-1}}{\partial W_{rec}} \cdots = \sum\limits_{k=0}^n \frac{\partial \xi}{\partial y} \frac{\partial y}{\partial S_k} \frac{\partial S_k}{\partial W_{rec}} = \sum\limits_{k=1}^n \frac{\partial \xi}{\partial S_k} S_{k-1} $ 3번 용어! 표본평균 / 표본분산" />
<meta property="og:site_name" content="AI Tech Study" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-08-08T07:01:56+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Week1 - 토요일" />
<script type="application/ld+json">
{"description":"1. 개인학습 베이즈 통계 조건부 확률을 활용해 정보를 갱신하는 방법 조건부 확률 $Y$ 가 관찰되었을 때, X가 발생할 확률 $ P(X \\vert Y) = {P(X,Y) \\over P(Y)} $ 베이즈 정리 $P(\\theta \\vert D) = P(\\theta) {P(D \\vert \\theta) \\over P(D)}$ $P(\\theta \\vert D)$ : 사후확률(posterior) $P(\\theta)$ : 사전확률(prior) $P(D \\vert \\theta)$ : 가능도(likelihood) $P(D)$ : Evidence 면접 복기 프로젝트 소개 프로젝트 관련 기초지식(ML/AI + CS) 이론 메트릭 Why? 2. 선택과제 복습 1번 미니배치 효과 불안정하지만 빠른 수렴 Convex 형태가 아닌 딥러닝 모델을 수렴시키기에 더 적합 2번 시간 역순으로 전파되어오는 그래디언트들의 합! $\\frac{\\partial \\xi}{\\partial W_x} = \\frac{\\partial \\xi}{\\partial y} \\frac{\\partial y}{\\partial S_n} \\frac{\\partial S_n}{\\partial W_x} + \\frac{\\partial \\xi}{\\partial y} \\frac{\\partial y}{\\partial S_n} \\frac{\\partial S_n}{\\partial S_{n-1}}\\frac{\\partial S_{n-1}}{\\partial W_x} \\cdots = \\sum\\limits_{k=0}^n \\frac{\\partial \\xi}{\\partial y} \\frac{\\partial y}{\\partial S_k} \\frac{\\partial S_k}{\\partial W_x} = \\sum\\limits_{k=0}^n \\frac{\\partial \\xi}{\\partial S_k} X_k $ $\\frac{\\partial \\xi}{\\partial W_{rec}} = \\frac{\\partial \\xi}{\\partial y} \\frac{\\partial y}{\\partial S_n} \\frac{\\partial S_n}{\\partial W_{rec}} + \\frac{\\partial \\xi}{\\partial y} \\frac{\\partial y}{\\partial S_n} \\frac{\\partial S_n}{\\partial S_{n-1}}\\frac{\\partial S_{n-1}}{\\partial W_{rec}} \\cdots = \\sum\\limits_{k=0}^n \\frac{\\partial \\xi}{\\partial y} \\frac{\\partial y}{\\partial S_k} \\frac{\\partial S_k}{\\partial W_{rec}} = \\sum\\limits_{k=1}^n \\frac{\\partial \\xi}{\\partial S_k} S_{k-1} $ 3번 용어! 표본평균 / 표본분산","mainEntityOfPage":{"@type":"WebPage","@id":"/ustage/2021/08/08/week1_SAT.html"},"author":{"@type":"Person","name":"LKH"},"@type":"BlogPosting","headline":"Week1 - 토요일","dateModified":"2021-08-08T07:01:56+09:00","datePublished":"2021-08-08T07:01:56+09:00","url":"/ustage/2021/08/08/week1_SAT.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


<script type="text/javascript" src="/assets/js/darkmode.js"></script>

</head>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        TeX: {
          equationNumbers: {
            autoNumber: "AMS"
          }
        },
        tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$', '$$'] ],
        processEscapes: true,
      }
    });
    MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
</script>
<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


<body>
  <main class="container">
    <section class="about">
      <div class="about-header condensed">
      <div class="about-title">
      <a href="/">
        
        <img src="/assets/portfolio.png" alt="KyungHyun Lim" />
        
      </a>
      <h2 id="title">
        <a href="/">KyungHyun Lim</a>
      </h2>
      </div><p class="tagline">AI/ML/SW Developer</p></div>
      
      <ul class="social about-footer condensed"><a href="https://github.com/KyungHyunLim" target="_blank">
          <li>
            <i class="icon-github-circled"></i>
          </li>
        </a><a href="mailto:fly1294@naver.com" target="_blank">
          <li>
            <i class="icon-mail-alt"></i>
          </li>
        </a></ul><nav class="navigation about-footer condensed">
        <ul>
          
          <li>
            <a href="/" class="ctext">Home</a>
          </li>
          
        </ul>
      </nav><p class="about-footer condensed">&copy;
        2021</p><div class="about-footer condensed">
        <p>Dark Mode
          <i class="icon-moon"></i>
          <label class="switch">
            <input type="checkbox" class="dark-mode-toggle">
            <span class="slider round" onclick="toggleDarkMode()"></span>
          </label>
        </p>
      </div>
    </section>
    <section class="content">
      <div class="post-container">
  <a class="post-link" href="/ustage/2021/08/08/week1_SAT.html">
    <h2 class="post-title">Week1 - 토요일</h2>
  </a>
  <div class="post-meta">
    <div class="post-date"><i class="icon-calendar"></i>Aug 8, 2021</div><ul class="post-categories"><li>ustage</li></ul></div>
  <div class="post">
    <h2 id="1-개인학습">1. 개인학습</h2>
<ul>
  <li>베이즈 통계
    <ul>
      <li>조건부 확률을 활용해 정보를 갱신하는 방법</li>
      <li>조건부 확률
        <ul>
          <li>$Y$ 가 관찰되었을 때, X가 발생할 확률</li>
          <li>$ P(X \vert Y) = {P(X,Y) \over P(Y)} $</li>
        </ul>
      </li>
      <li>베이즈 정리
        <ul>
          <li>$P(\theta \vert D) = P(\theta) {P(D \vert \theta) \over P(D)}$</li>
          <li>$P(\theta \vert D)$ : 사후확률(posterior)</li>
          <li>$P(\theta)$ : 사전확률(prior)</li>
          <li>$P(D \vert \theta)$ : 가능도(likelihood)</li>
          <li>$P(D)$ : Evidence</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>면접 복기
    <ul>
      <li>프로젝트 소개</li>
      <li>프로젝트 관련 기초지식(ML/AI + CS)
        <ul>
          <li>이론</li>
          <li>메트릭</li>
          <li>Why?</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="2-선택과제-복습">2. 선택과제 복습</h2>
<ul>
  <li>1번
    <ul>
      <li>미니배치 효과
        <ul>
          <li>불안정하지만 빠른 수렴</li>
          <li>Convex 형태가 아닌 딥러닝 모델을 수렴시키기에 더 적합</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>2번
    <ul>
      <li>시간 역순으로 전파되어오는 그래디언트들의 합!</li>
      <li>$\frac{\partial \xi}{\partial W_x} = \frac{\partial \xi}{\partial y} \frac{\partial y}{\partial S_n} \frac{\partial S_n}{\partial W_x} + \frac{\partial \xi}{\partial y} \frac{\partial y}{\partial S_n} \frac{\partial S_n}{\partial S_{n-1}}\frac{\partial S_{n-1}}{\partial W_x} \cdots = \sum\limits_{k=0}^n \frac{\partial \xi}{\partial y} \frac{\partial y}{\partial S_k} \frac{\partial S_k}{\partial W_x} = \sum\limits_{k=0}^n \frac{\partial \xi}{\partial S_k} X_k $</li>
      <li>$\frac{\partial \xi}{\partial W_{rec}} = \frac{\partial \xi}{\partial y} \frac{\partial y}{\partial S_n} \frac{\partial S_n}{\partial W_{rec}} + \frac{\partial \xi}{\partial y} \frac{\partial y}{\partial S_n} \frac{\partial S_n}{\partial S_{n-1}}\frac{\partial S_{n-1}}{\partial W_{rec}} \cdots = \sum\limits_{k=0}^n \frac{\partial \xi}{\partial y} \frac{\partial y}{\partial S_k} \frac{\partial S_k}{\partial W_{rec}} = \sum\limits_{k=1}^n \frac{\partial \xi}{\partial S_k} S_{k-1} $</li>
    </ul>
  </li>
  <li>3번
    <ul>
      <li>용어! 표본평균 / 표본분산</li>
    </ul>
  </li>
</ul>

  </div></div>

    </section>
    <footer class="condensed">
      <ul class="social about-footer condensed"><a href="https://github.com/KyungHyunLim" target="_blank">
          <li>
            <i class="icon-github-circled"></i>
          </li>
        </a><a href="mailto:fly1294@naver.com" target="_blank">
          <li>
            <i class="icon-mail-alt"></i>
          </li>
        </a></ul><nav class="navigation about-footer condensed">
        <ul>
          
          <li>
            <a href="/" class="ctext">Home</a>
          </li>
          
        </ul>
      </nav><p class="about-footer condensed">&copy;
        2021</p><div class="about-footer condensed">
        <p>Dark Mode
          <i class="icon-moon"></i>
          <label class="switch">
            <input type="checkbox" class="dark-mode-toggle">
            <span class="slider round" onclick="toggleDarkMode()"></span>
          </label>
        </p>
      </div>
    </footer>
  </main>
  
  <script type="text/javascript" src="/assets/js/darkmode.js"></script>
  
</body>

</html>
