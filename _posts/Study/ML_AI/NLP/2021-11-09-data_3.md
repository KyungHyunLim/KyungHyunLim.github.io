---
layout: post
title:  "자연어처리 데이터 소개"
date:   2021-11-09 15:17:00
categories: [Pytorch, ML_AI]
use_math: true
---

# 0. 강의 소개
* 국내의 자연어처리 데이터 제작 흐름에 대해 알아봅니다.
    * 국립국어원의 21세기 세종 계획, ETRI의 엑소브레인, 그리고 최근 공개된 모두의 말뭉치에 대해 알아봅니다.
    * 국가 주도 데이터셋 구축과 대비되는, 민간 주도 데이터셋 구축에 대해서도 알아봅니다.
* 다양한 자연어처리 데이터의 유형에 대해 알아봅니다.
* 한국어에 국한되지 않고, 대표적으로 활용되는 자연어처리 태스크 및 데이터를 소개합니다.
    * 질의응답, 기계번역, 대화, 요약 등의 태스크들에 대해 알아봅니다.
    * 각 태스크의 대표적인 벤치마크 데이터들에 대해 알아봅니다.

# 1. 국내 언어 데이터의 구축 프로젝트
![](/assets/image/data/3_1.PNG)  
* 21세기 세종 계획
    * '21세기 세종계획'은 1997년에 그 계획이 수립되었고 이듬해인 1998년부터 2007년까지 10년 동안 시행된 한국의 국어 정보화 중장기 발전 계획(홍윤표,U2009)U총 2억 어절의 자료 구축,U공개 
    * XMLU형식,U언어정보나눔터 누리집을 통해 배포하다 중단 후 DVD로만 배포
* 모두의 말뭉치
    * 인공지능의 한국어 처리 능력 향상에 필수적인 한국어 학습 자료 공개 플랫폼.U‘21세기 세종계획’에 비해 일상 대화,U메신저,U웹 문서 등 구어체 자료의 비중을 높임.U다층위 주석 말뭉치 포함(형태,U구문,U어휘 의미,U의미역,U개체명,U상호 참조 등) 
    * JSONU형식,U모두의 말뭉치 누리집(https://corpus.korean.go.kr/)에서 배포
* 엑소 브레인  
    ![](/assets/image/data/3_2.PNG) 

# 2. 질의응답
* SQuAD
    * 위키피디아 데이터를 기반으로 제작한 기계독해 및 질의응답 데이터
        * [데이터 링크](https://rajpurkar.github.io/SQuADexplorer/)
    * 기사, 질문과 답변으로 구성
* SQuAD 1.0 구축 과정
    1. 구축 대상 기사 추출
        * 위키피디아 상위 10,000 기사 중 500자 이하인 536 기사 무작위 추출
    2. 크라우드 소싱을 통한 질의 응답 수집
        * 각 문단마다 다섯 개의 질문과 답변 수집
    3. 추가 응답 수집
        * 평가를 통해서 각 질문 당 최소 두 개의 추가적인 답변 수집. 기사의 단락과 질문 노출 후 가장 짧은 대답 선택
* SQuAD 2.0 구축 과정
    1. 크라우드 소싱 플랫폼을 통한 대답하기 어려운 질문 수집
        * 각 문단 마다 문단만으로는 대답할 수 없는 다섯개의 질문 생성
        * 적합한 질문을 25개 이하로 남김
    2. 적합한 질문이 수집되지 않은 기사 삭제
    3. 학습, 검증, 평가용 데이터 분할

# 3. 기계번역
* WMT
    * 2014년 부터 시행된 기계 번역 학회에서 공개한 데이터셋 다국어 번역 데이터
* 구축 과정
    * 평가용 데이터: 1,500개의 영어 문장을 다른 언어로 번역 + 1,500개의 문장은 다른 언어에서 영어 문장으로 번역
    * 훈련용 데이터: 기존에 존재하는 병렬 말뭉치와 단일 언어 말뭉치를 제공

# 4. 요약
* CNN/Daily Mail
    * 추상 요약 말뭉치. 기사에 대하여 사람이 직접 작성한 요약문이 짝을 이루고 있음
    * 학습: 286,817, 검증: 13,368, 평가: 11,487
    * 저작권 문제로 URL list 제공
    * [데이터 링크](https://github.com/abisee/cnn-dailymail)

# 5. 대화
* DSTC - Dialog System Technology Challenges
    * human-computer dialogs in the restaurant information domain
    * human-human dialogs in the tourist information domain
    * End-to-End Goal Oriented Dialog Learning, End-to-End Conversation Modeling, and Dialogue Breakdown Detection로 확장
* Wizard-of-Oz
    * WoZ 방식으로 수집된 데이터셋이며 대화 상태 추적 데이터와 유사한 형태로 이루어짐
    * Woz 방식
        * 대화 수집 방식 중 하나, 참여자가 대화 시스템을 통해 대화를 하고 있다고 생각하게 한 뒤 실제로는 사람이 참여자의 발화에 맞추어 응답을 제시하고 이끌어낸 대화를 수집
* Ubuntu Dialogue Corpus
    * 우분투 플랫폼 포럼의 대화를 수집
    * 100만 개의 멀티 턴 대화로 구성
    * 대화 상태 추적과 블로그 등에서 보이는 비구조적 상호작용의 특성을 모두 가지고 있음

# 6. Further Questions
* 형태소 분석, 의존구문 분석 등을 위한 코퍼스와, 감정 분석, 유사도 분석 등을 위한 코퍼스는 어떤 경향의 차이를 보일까요?
* KLUE, KorQuAD와 같은 벤치마크에서 보완할 수 있는 점은 무엇이 있을까요?
* 데이터셋을 제작하는 데 있어 한국어 데이터 수집의 예상되는 어려운점은 무엇이 있나요?
* 질의응답 및 기계 번역 태스크가 활발하게 연구되기 시작한 배경은 무엇일까요?
* Open-domain 대화 데이터셋과 Task-oriented 대화 데이터셋을 구축할 때의 어려운 점은 어떻게 다를까요?

# 7. Further Reading
* [Open Korean Corpora](https://www.preprints.org/manuscript/202110.0247/v1)
* [A Survey on Awesome Korean NLP Datasets](https://aclanthology.org/2020.nlposs-1.12/)
* [Hugging Face Dataset Hub](https://huggingface.co/datasets)
* [Papers with Code - NLP section](https://paperswithcode.com/area/natural-language-processing)

# Reference
* AI boot camp 2기 데이터 제작 강의