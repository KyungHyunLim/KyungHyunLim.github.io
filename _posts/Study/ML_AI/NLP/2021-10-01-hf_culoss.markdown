---
layout: post
title:  "Hugging face - Custom Loss"
date:   2021-10-01 10:39:22
categories: [NLP, ML_AI]
use_math: true
---

# 1. transformers - Trainer class 'compute_loss' ì‚´í´ë³´ê¸°
* transformersì˜ Trainer classë¥¼ gitì— ê°€ì„œ ì‚´í´ë³´ë©´, compute_loss í•¨ìˆ˜ê°€ ì–´ë–»ê²Œ ì •ì˜ë˜ì—ˆë‚˜ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤!
* https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py [1864 line]

```python
def compute_loss(self, model, inputs, return_outputs=False):
        # ê¸°ë³¸ì ìœ¼ë¡œ hugging faceì—ì„œ ì œê³µí•˜ëŠ” ëª¨ë“  ëª¨ë¸ë“¤ì€ ì²«ë²ˆì§¸ ì›ì†Œì— lossë¥¼ ë°˜í™˜ í•œë‹¤ê³  ì„¤ëª…í•˜ê³  ìˆìŠµë‹ˆë‹¤.
        """
        How the loss is computed by Trainer. By default, all models return the loss in the first element.
        Subclass and override for custom behavior.
        """
        
        # label_smoother ê°€ None ì´ê³ , labelsê°€ inputsì— ì¡´ì¬í•˜ë©´ labelsë¥¼ êº¼ë‚´ ì˜µë‹ˆë‹¤.
            # labelì´ ì—†ëŠ” taskê°€ ìˆê¸° ë•Œë¬¸ì´ ì•„ë‹Œê°€ ì‹¶ìŠµë‹ˆë‹¤! -> ë” ë¶„ì„ì´ í•„ìš”í• ë“¯
        if self.label_smoother is not None and "labels" in inputs:
            labels = inputs.pop("labels")
        else:
            labels = None

        # ëª¨ë¸ì— ì…ë ¥ì„ ë„£ì–´ outputì„ ìƒì„±í•˜ê³ ,
        outputs = model(**inputs)

        # í  ë‚˜ì¤‘ì— ê³ ì¹  í•„ìš”ê°€ ìˆë‹¤ê³  í•˜ë„¤ìš”. ê³¼ê±° stateê°€ í•„ìš”í•œ taskì—ì„œ ì‚¬ìš©ì´ ë˜ëŠ”ê²ƒ ê°™ìŠµë‹ˆë‹¤.
        # Save past state if it exists 
        # TODO: this needs to be fixed and made cleaner later.
        if self.args.past_index >= 0: 
            # ì¸ë±ìŠ¤ì— ë§ì¶°ì„œ ê³¼ê±° ouputì„ ë‹¤ ì €ì¥í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì–´ë”˜ê°€ì—ì„œ ë”°ë¡œ ì‚¬ìš©ì´ ë˜ëŠ”ë“¯í•©ë‹ˆë‹¤. ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„  ìƒê´€ì—†ëŠ” ë¶€ë¶„!
            self._past = outputs[self.args.past_index]

        # labelì´ ìˆìœ¼ë©´ lossë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        if labels is not None:
            # ê¸°ë³¸ì ìœ¼ë¡œëŠ” label_smoother ë¼ëŠ” lossë¥¼ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.
            loss = self.label_smoother(outputs, labels)
        else:
            # ì´ë¶€ë¶„ì€ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ”ë‹¤ê³  í•˜ë„¤ìš”
            # We don't use .loss here since the model may return tuples instead of ModelOutput.
            loss = outputs["loss"] if isinstance(outputs, dict) else outputs[0]

        # ê³„ì‚°í•œ ê°’ë“¤ì„ ë°˜í™˜í•´ ì¤ë‹ˆë‹¤!
        return (loss, outputs) if return_outputs else loss
```

# 2. Trainer ìƒì† ë°›ì•„ compute_loss ì˜¤ë²„ë¼ì´ë”© í•˜ê¸°
* Overridingì„ í•´ì•¼í•˜ëŠ”ë°, ë¨¼ì € Trainer classë¥¼ ìƒì†ë°›ëŠ” Mytrainerë¥¼ ì •ì˜í•´ì•¼í•©ë‹ˆë‹¤.
* initì—ì„œëŠ” ë¶€ëª¨ classì¸ Trainerì— ê¸°ì¡´ì— ì‚¬ìš©í•˜ë˜ argsë“¤ì„ ë„˜ê²¨ì£¼ì–´ ê¸°ëŠ¥ë“¤ì„ ë‹¤ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì´ˆê¸°í™” í•´ì¤ë‹ˆë‹¤
* ì¶”ê°€ì ìœ¼ë¡œ ì›í•˜ëŠ” lossë¥¼ ì„¤ì •í•˜ê¸° ìœ„í•´ loss_nameì„ ì¶”ê°€ë¡œ ë°›ì•„ ì£¼ê² ìŠµë‹ˆë‹¤!

    ```python
    class MyTrainer(Trainer):
        # loss_name ì´ë¼ëŠ” ì¸ìë¥¼ ì¶”ê°€ë¡œ ë°›ì•„ selfì— ê°ì¸ ì‹œì¼œì¤ë‹ˆë‹¤.
        def __init__(self, loss_name, *args, **kwargs):
            super().__init__(*args, **kwargs)
            self.loss_name= loss_name # ê°ì¸!

        def compute_loss(self, model, inputs, return_outputs=False):
    ```

* ë³¸ê²©ì ìœ¼ë¡œ compute_lossë¥¼ ë³€ê²½í•´ ë³´ê² ìŠµë‹ˆë‹¤.!

    ```python
    def compute_loss(self, model, inputs, return_outputs=False):

        # configì— ì €ì¥ëœ loss_nameì— ë”°ë¼ ë‹¤ë¥¸ loss ê³„ì‚° 
        if self.loss_name == 'CrossEntropy':
            # lossnameì´ CrossEntropy ì´ë©´, custom_lossì— torch.nn.CrossEntropyLoss()ë¥¼ ì„ ì–¸(?) í•´ì¤ë‹ˆë‹¤.
            custom_loss = torch.nn.CrossEntropyLoss()
                    
        if self.label_smoother is not None and "labels" in inputs:
            labels = inputs.pop("labels")
        else:
            labels = None

        outputs = model(**inputs)

        if labels is not None:
            #lossë¥¼ ê³„ì‚° í•˜ë˜ ë¶€ë¶„ì— custom_lossë¥¼ ì´ìš©í•´ ê³„ì‚°í•˜ëŠ” ì½”ë“œë¥¼ ë„£ì–´ ì¤ë‹ˆë‹¤!
            #ì›ë³¸ ì½”ë“œë¥¼ ë³´ì‹œë©´ output[0]ê°€ logit ì„ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤!
            loss = custom_loss(outputs[0], labels)
        else:
            # We don't use .loss here since the model may return tuples instead of ModelOutput.
            loss = outputs["loss"] if isinstance(outputs, dict) else outputs[0]
        return (loss, outputs) if return_outputs else loss
    ```

* ì ì´ì œ ìƒˆë¡œìš´ lossë¥¼ ì ìš©í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤!

# 3. ì ìš©í•˜ê¸°!
* MyTrainer ê°ì²´ë¥¼ ì„ ì–¸í•  ë•Œ, ì›ë˜ ì‚¬ìš©í•˜ë˜ argë“¤ê³¼ ì¶”ê°€ë¡œ ì •ì˜í•œ loss_nameì„ ë„˜ê²¨ì¤ë‹ˆë‹¤.
* ë‹¤ìŒì—ëŠ”, í•˜ì‹œë˜ë°ë¡œ trainì„ í˜¸ì¶œ í•˜ë©´ ë!

    ```python
    # Custom Loss ì‚¬ìš©ì„ ìœ„í•´ Trainner ì •ì˜ (loss.py)
    trainer = MyTrainer(
        loss_name='CrossEntropy',            # ë‚´ê°€ ìƒˆë¡­ê²Œ ì •ì˜í•œ / ë˜ëŠ” Torchì— ìˆëŠ” ë‹¤ì–‘í•œ Lossë“¤ ì ìš©ê°€ëŠ¥
        model=model,                         # the instantiated ğŸ¤— Transformers model to be trained
        args=training_args,                  # training arguments, defined above
        train_dataset=train_dataset,         # training dataset
        eval_dataset=valid_dataset,          # evaluation dataset
        compute_metrics=compute_metrics      # define metrics function
    )

    # train model
    trainer.train()

    Optimizers = (Optimzer, Scheduler) argë¥¼ ì´ìš©í•˜ë©´ Optimzer, Scheduler ë„ ììœ ë¡­ê²Œ ë³€ê²½í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤!
    ```
