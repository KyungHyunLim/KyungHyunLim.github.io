---
layout: post
title:  "BERT 언어모델 소개 및 학습 with 한국어"
date:   2021-09-28 10:34:22
categories: [NLP, ML_AI]
use_math: true
---

# 1. BERT 언어모델
## 1.1 Intro
![](/assets/image/plv2/w9_d2_1.PNG)
* Seq2Seq의 단점을 보완하기 위해 Seq2Seq + Attention, 최신 모델인 transformer로 발전중
![](/assets/image/plv2/w9_d2_2.PNG)
* GPT-1 $\rightarrow$ BERT $\rightarrow$ GPT-2 순으로 모델 제안
* BERT  
    ![](/assets/image/plv2/w9_d2_3.PNG)
    * [CLS] token은 sentence의 모든 정보가 담김
    * Masked Language Model
* Dataset
    * [GLUE data, task](https://kyunghyunlim.github.io/nlp/ml_ai/2021/09/22/hugging_face_5.html) 
    * SQuAD v1.1 질의응답
    * CoNLL 2003 개체명 분류
    * SWAG 다음에 이어질 자연스러운 문장 선택

## 1.2 BERT 응용
* 감정 분석  
![](/assets/image/plv2/w9_d2_4.PNG)

* 관계 추출  
![](/assets/image/plv2/w9_d2_5.PNG)

* 의미 비교  
![](/assets/image/plv2/w9_d2_6.PNG)

* 개체명 분석  
![](/assets/image/plv2/w9_d2_7.PNG)

* 기계 독해  
![](/assets/image/plv2/w9_d2_8.PNG)

## 1.3 한국어 BERT 모델
* ETRI
    * 형태소단위로 분리 후, Word piece 적용  
    ![](/assets/image/plv2/w9_d2_9.PNG)
* [스캐터랩](https://arxiv.org/abs/2010.02534)
* Advanced BERT model
    * 중요한 정보에 따라 전처리를 잘 해주는 것이 중요
    ![](/assets/image/plv2/w9_d2_10.PNG)

# 2. BERT 기반 단일 문장 분류 모델
* [관계 추출 포스트](https://kyunghyunlim.github.io/nlp/ml_ai/2021/09/24/hugging_face_6.html)

# 3. BERT Special Token 추가하기
* [참고](https://github.com/huggingface/tokenizers/issues/247)

```python
# 추가 하고 싶은 Special token dict 정의
special_tokens_dict = {'additional_special_tokens': ['[C1]','[C2]','[C3]','[C4]']}
# tokenizer에 더해주기
num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)
# 모델도 늘려줘야함!
model.resize_token_embeddings(len(tokenizer))
```