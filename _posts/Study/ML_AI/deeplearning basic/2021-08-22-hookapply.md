---
layout: post
title:  "Hook and Apply"
date:   2021-08-22 15:52:12
categories: [Pytorch, ML_AI]
use_math: true
---

## 1. Intro
 applyëŠ” ë§ì´ ì“°ì´ì§€ë§Œ hookì€ ì‚¬ì‹¤ ì˜ ì“°ì´ëŠ” ê¸°ëŠ¥ì€ ì•„ë‹ˆë¼ê³  í•œë‹¤. í•˜ì§€ë§Œ ì•Œì•„ë‘ë©´ ì–¸ì  ê°€ ìœ ìš©í•  ê²ƒ ê°™ë‹¤. hookì„ ì´ìš©í•´ì„œ gradient, forward ê³„ì‚°ê°’ë“± ë‹¤ì–‘í•œ ì¡°ì ˆì´ ê°€ëŠ¥í•˜ê³ , ê·¸ê±¸ applyë¥¼ í†µí•´ ì›í•˜ëŠ” ëª¨ë“ˆì—ë§Œ ì ìš©í•˜ê²Œë„ ë§Œë“¤ ìˆ˜ ìˆë‹¤. ìµœê·¼ pretrained ëª¨ë¸ì„ ë§ì´ ì‚¬ìš©í•˜ëŠ”ë°, ìƒˆë¡œìš´ ì‹œë„ë¥¼ ì ìš©í•´ë³´ê¸° ì ì ˆí•œ ê¸°ëŠ¥ë“¤ì¸ê²ƒ ê°™ë‹¤.

## 2. hook
    * í”„ë¡œê·¸ë¨ì˜ ì‹¤í–‰ ë¡œì§ì„ ë¶„ì„í•˜ê±°ë‚˜
    * í”„ë¡œê·¸ë¨ì— ì¶”ê°€ì ì¸ ê¸°ëŠ¥ì„ ì œê³µí•˜ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©
    * Forward hook

    ```python
    # TODO: ë‹µì„ x1, x2, output ìˆœì„œë¡œ listì— ì°¨ë¡€ì°¨ë¡€ ë„£ìœ¼ì„¸ìš”! 
    answer = []

    # TODO : pre_hookë¥¼ ì´ìš©í•´ì„œ x1, x2 ê°’ì„ ì•Œì•„ë‚´ answerì— ì €ì¥í•˜ì„¸ìš”
    def pre_hook(module, input):
        for v in input:
            answer.append(v)
        pass
    # TODO : hookë¥¼ ì´ìš©í•´ì„œ output ê°’ì„ ì•Œì•„ë‚´ answerì— ì €ì¥í•˜ì„¸ìš”
    def hook(module, input, output):
        answer.append(output)
        pass
    add.register_forward_pre_hook(pre_hook)
    add.register_forward_hook(hook)
    --------------------------------------------------------------------
    # TODO : hookë¥¼ ì´ìš©í•´ì„œ ì „íŒŒë˜ëŠ” output ê°’ì— 5ë¥¼ ë”í•´ë³´ì„¸ìš”!
    def hook(module, input, output):
        output = output + 5
        return output
        pass

    add.register_forward_hook(hook)
    ```

    * Backward hook

    ```python
    # TODO: ë‹µì„ x1.grad, x2.grad, output.grad ìˆœì„œë¡œ listì— ì°¨ë¡€ì°¨ë¡€ ë„£ìœ¼ì„¸ìš”! 
    answer = []

    # TODO : hookë¥¼ ì´ìš©í•´ì„œ x1.grad, x2.grad, output.grad ê°’ì„ ì•Œì•„ë‚´ answerì— ì €ì¥í•˜ì„¸ìš”
    def module_hook(module, grad_input, grad_output):
        for gi in grad_input:
            answer.append(gi)
        answer.append(grad_output[0])
        pass

    model.register_full_backward_hook(module_hook)
    ---------------------------------------------------------------
        
    # TODO : hookë¥¼ ì´ìš©í•´ì„œ Wì˜ gradient ê°’ì„ ì•Œì•„ë‚´ answerì— ì €ì¥í•˜ì„¸ìš”
    def tensor_hook(grad):
    #     print(grad)
        answer.append(grad)
        pass

    model.W.register_hook(tensor_hook)
    ```
## 3. apply
    * apply í•¨ìˆ˜ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”(Weight Initialization)ì— ë§ì´ ì‚¬ìš©
    * applyë¥¼ í†µí•´ ì ìš©í•˜ëŠ” í•¨ìˆ˜ëŠ” ëª¨ë“  moduleë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì…ë ¥ë°›ì•„ì„œ ì²˜ë¦¬

    ```python
    # TODO : applyë¥¼ ì´ìš©í•´ ëª¨ë“  Parameter ê°’ì„ 1ë¡œ ë§Œë“¤ì–´ë³´ì„¸ìš”!
    def weight_initialization(module):
        module_name = module.__class__.__name__
        if module_name.split('_')[0] == 'Function':
            module.W = torch.nn.Parameter(torch.tensor([1.0]), requires_grad=True)
            #print(type(module))

    # ğŸ¦† applyëŠ” applyê°€ ì ìš©ëœ moduleì„ return í•´ì¤˜ìš”!
    returned_module = model.apply(weight_initialization)
    ```
    * ê¸°ì¡´ ëª¨ë¸ì— íŒŒë¼ë¯¸í„° ì¶”ê°€í•˜ê³ , forward ë°©ì‹ ë°”ê¾¸ê¸°

    ```python
    # TODO : applyë¥¼ ì´ìš©í•´ Parameter bë¥¼ ì¶”ê°€í•´ë³´ì„¸ìš”!
    def add_bias(module):
        module_name = module.__class__.__name__
        # ê° Function ëª¨ë“ˆì— bias ì¶”ê°€
        if module_name.split('_')[0] == "Function":
            module.b = Parameter(torch.rand(2, ), requires_grad=True)
            

    # TODO : applyë¥¼ ì´ìš©í•´ ì¶”ê°€ëœ bë„ ê°’ì„ 1ë¡œ ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”!
    def weight_initialization(module):
        module_name = module.__class__.__name__

        if module_name.split('_')[0] == "Function":
            module.W.data.fill_(1.)
            # Function ëª¨ë“ˆ bias ì´ˆê¸°í™”
            module.b.data.fill_(1.)


    # TODO : applyë¥¼ ì´ìš©í•´ ëª¨ë“  Functionì„ linear transformationìœ¼ë¡œ ë°”ê¿”ë³´ì„¸ìš”!
    #        X @ W + b
    def linear_transformation(module):
        module_name = module.__class__.__name__
        
        # hookì„ ì´ìš©í•´ forward í•¨ìˆ˜ ì¶œë ¥ê°’ ì„ í˜•ìœ¼ë¡œ ë³€í˜•
        def hook(moudle, input, output):
            output = (torch.matmul(input[0], module.W.T) + module.b)
            return output
        
        # ê° ëª¨ë“ˆì— hook ë“±ë¡
        if module_name == "Function_A":
            module.register_forward_hook(hook)
        elif module_name == "Function_B":
            module.register_forward_hook(hook)
        elif module_name == "Function_C":
            module.register_forward_hook(hook)
        elif module_name == "Function_D":
            module.register_forward_hook(hook)
            
    returned_module = model.apply(add_bias)
    returned_module = model.apply(weight_initialization)
    returned_module = model.apply(linear_transformation)

    ```