---
layout: post
title:  "Softmax regression and Cross-entropy"
date:   2021-08-14 13:55:22
categories: [ML_AI]
use_math: true
---

## 1. Softmax regression
### 1.1 One-hot encoding
* 분류하고 싶은 class만큼의 차원을 만들어 해당하는 부분을 1로 셋팅
    * E.g 개, 고양이, 말을 분류하고 싶으면
    * 개 = (1, 0, 0)
    * 고양이 = (0, 1, 0)
    * 말 = (0, 0, 1)
* 원핫 인코딩 벡터를 Lable, 정답으로 

### 1.2 Softmax
* $ {e^{x_i} \over \sum e^{x_i} }$
    * 다른 클래스에 비해 i번째 클래스일 확률을 알 수 있다.
    * 즉, 상대평가가 가능
* Exponential을 활용하는 이유?
    * 계산했을때, 차이가 더 두드러지기 때문

## 2. Cross-entropy with Softmax
* Cross-entropy
    * $-\sum_i p_i log(q_i) $
        * $p_i$ : 내가 원하는 확률
        * $q_i$ : 계산해서(예측한) 확률
    * 위 식을 minimize하면 $q_i$ 가 $p_i$ 와 가까워진다.
    * $q_i$ 에 Softmax 함수 적용
* 학습 원리  
 결국 $p_i$ 는 해당 클래스일 확률, 즉 정답지 이기 때문에 1이다. 따라서 Softmax가 적용된 $q_i$ 는 나머지 클래스일 확률들을 최대한 줄이고 정답 클래스일 확률 값(마지막 레이어의 출력)을 최대한 높이는 방향으로 Weight를 업데이트하게 된다.  
 여기서 조금더 살펴보면, Cross-entropy Loss에서 $q_i$ 에 log를 취했고, 0 ~ 1 사이의 값을 가질 수 있기 때문에, Cross-entropy를 최소화 한다는 의미는 $q_i$ 의 값이 1로 수렴해야 한다는 말이 된다. (1 미만의 값에서는 -가 붙어있어 양수가 되어버리 기 때문!!)