---
layout: post
title:  "Generative model-VAE, GAN"
date:   2021-08-13 13:55:22
categories: [ML_AI]
use_math: true
---

## 1. Latent Variable Models
### 1.1 Variational Inference and Deep Learning
* Autoencoder는 generative 모델일까?  
 Autoencoder는 gnerative 모델이 아니다! Latent space를 샘플링 해서 디코더를 거쳐 데이터를 생성할 수 있어야 한다.

### 1.2 Variational inference
* VI의 목표: posterior distribution과 매칭되는 variational distrubution을 최적화 하는것  
    ![](/assets/image/ML_AI/gm2_1.PNG)
    * Posterior distribution: $P_\theta (z \vert x)$ (뒤집힌건 likelihood)
    * Variational distribution: $Q_\phi (z \vert x)$
    * 보통 P를 계산하기 어렵기 때문에 P와 유사한 Q를 만드는 것이 VI의 과정이다.
* 문제는?
    * Posterior distribution를 모른다!
    * ELBO trick이 이것을 가능하게 해준다.  
    ![](/assets/image/ML_AI/gm2_2.PNG)
    * VI의 목적인 q와 p사이에 KL divergence를 줄이는 것이 목적인데, 이것이 불가능 하기 때문에, ELBO를 커지게하는 것으로 반대로 원하는 것을 얻고자 하는 것.   
    ![](/assets/image/ML_AI/gm2_3.PNG)
    * Evidence Lower Bound(ELBO)는 계산할 수 있다! ELBO 식을 잘 유도해보면 Reconstruction Term과 Prior Fitting Term으로 변환할 수 있다. Reconstruction Term은 입력 x가 인코더를 거쳐 z를 생성하고, 이 z가 디코더를 통과해 다시 x가 됬을 때의 loss를 의미한다. Prior Fitting Term은 데이터들을 latent space에 올려두었을 때, 데이터들이 이루는 분포가 내가 가정하는 사전 분포와 유사한지에 대한 loss를 의미한다. 즉, 궁극적으로 x를 잘 표현할 수 있는 latent space z를 찾고 싶은 것. 
* Key limitation
    * intractable (likelihood를 계산하기 어렵다)
    * 대부분 gaussian 사전 분포를 활용

### 1.3 Adversaril Autoencoder
![](/assets/image/ML_AI/gm2_4.PNG)
* Gaussian 분포를 활용하고 싶지 않다!
* GAN을 활용해 Latent distribution 사이 분포를 맞춰 주는것
    * Smapling 가능한 분포가 있으면 이에 z를 맞출 수 있다
    * 복잡하고 다양한 분포를 활용 할 수 있어짐

## 2. Generative Adversarial Network
### 2.1 Concept
![](/assets/image/ML_AI/gm2_5.PNG)
* GAN은 도둑이 위조 지폐를 만들고 싶은데, 위조 지폐를 잘 감별할 수 있는 경찰이 있다. 이때 도둑이 경찰을 속이기 위해 위조 지폐 기술을 계속 발전시키고, 경찰 또한 탐지 기술을 계속 발전시키면서 서로 고도화 되어가는 과정과 동일하다.
* $min_G max_D V(D, G)=E_{x \sim p_{data}(x)}[logD(x)] + E_{z \sim p_z(z)}[log(1-D(G(z)))]$
    * $min_G$ : $D(G(z))=1$ 일때 최소가 된다
    * $max_D$ : $D(x)=1, \ D(G(z))=0$ 일때 최대가 된다

### 2.2 VAE vs GAN
![](/assets/image/ML_AI/gm2_6.PNG)
* VAE  
 학습단계에서 x를 인코더를 통해 latent space z로 보내지고, 이 z를 활용해 디코더는 x를 생성한다. 추론 단계에서는 P(z)로 부터 샘플링한 z를 기반으로 x를 생성하는 방식으로 사용이 된다.
* GAN  
 학습단계에서 실제 데이터와 G가 z로 부터 생성한 이미지를 Real과 Fake라는 라벨로 분리해 Discriminator를 학습 시키고, G가 생성한 이미지가 D에서 Real로 판별되도록 학습시킨다. 따라서 학습이 거듭되면 G는 점점 실제와 유사하게 이미지를 생성하도록 성장한다. 추론 단계에서는 G만 이용해 데이터를 생성한다.

### 2.3 GAN Objective
* generator와 discriminator의 minimax 게임
* Obtimal discriminator
    * $D_G^{*}(x)={P_{data}(x) \over P_{data}(x) + P_G(x)}$
    * 위 식을 GAN의 object function에 대입하면,  
    ![](/assets/image/ML_AI/gm2_7.PNG)
    * Jenson-Shannon Divergence를 최소화 하는 것과 동일해진다.

## 3. GAN 종류
### 3.1 DCGAN
![](/assets/image/ML_AI/gm2_8.PNG)
* Leaky ReLU의 유용성
* 이미지를 만들때는 어떤식으로 구성하는게 좋더라를 제안

### 3.2 Info-GAN
![](/assets/image/ML_AI/gm2_9.PNG)
* 특정 정보에 집중하도록 C를 추가

### 3.3 Text2Image
![](/assets/image/ML_AI/gm2_14.PNG)

### 3.4 Puzzle-GAN
![](/assets/image/ML_AI/gm2_10.PNG)
* sub patch들을 가지고 GAN을 이용해 이미지 복원
* E.g 범퍼, 헤드라이트, 차체 => 자동차

### 3.5 Cycle-GAN
![](/assets/image/ML_AI/gm2_11.PNG)
* 이미지 Translation
* Cycle-consistency loss
    * 매칭이 되는 두 이미지가 없어도 학습 가능

### 3.6 Star-GAN
![](/assets/image/ML_AI/gm2_12.PNG)
* 이미지를 컨트롤 할 수 있도록 학습

### 3.6 Progressive-GAN
![](/assets/image/ML_AI/gm2_13.PNG)
![](/assets/image/ML_AI/gm2_15.gif)
* 고차원의 이미지를 잘 만들어준다.

