---
layout: post
title:  "GAN - Mode collapsing"
date:   2021-08-14 16:14:28
categories: [ML_AI]
use_math: true
---

## 1. Vanila GAN Backpropagation in Paper
![](/assets/image/ML_AI/gp_1.PNG)
* Minibatch SGD를 optimizer로 학습을 진행한다.
* Discriminator를 SGD가 높아지는 방향으로 업데이트 한다.
    * 식을 살펴보면 실제 데이터를 실제 데이터로 잘 판단하면 $log 1$ 로 0이 되고, 가짜 데이터를 가짜 데이터로 잘 판단하면 $log (too_small_number) $ 로 역시나 $log 1$ 로 0이 된다.  
    반대의 경우, log 값들은 매우 작아지게 된다. 따라서 Discriminator는 ascending 하는 방향으로 업데이트를 진행한다.
* Generator는 SGD가 감소하는 방향으로 업데이트 한다.
    * 논문의 식을 보면, Generator가 생성하는 데이터가 엉망이어서 D를 속이기 어렵다면$log(1-0.xx)$ 가 되어 0에 가까워 진다.  
    반대로, 매우 잘 속인다면 $log(1-1)$ 로 매우 작은 값을 가지게 될 것이다. 따라서 Generator는 SGD가 descending 하는 방향으로 업데이트 해 나아간다.

## 2. GAN의 Vanishing Gradient 문제
* Mode collapse problem
    * GAN의 BCE loss를 사용할 경우 생기는 문제 중 하나
    * GAN이 다양한 데이터를 만들어내지 못하고, 비슷한 데이터만 계속해서 생성하는 문제
    * 즉, 생성자가 local minimum에 빠진것!
* 보통 판별자가 생성자 보다 학습이 빠르다!
    * 따라서 학습이 어느정도 진행된 경우, D는 데이터가 가짜인지 실제인지 매우 잘 구분하게 된다. 결국 Vnila GAN에서는 이러한 상황에서 논문에서 주어진 Gradient 값이 0이 되는 것을 확인 할 수 있다. 따라서 생성자의 Update가 잘 이루어지지 않고, 덜 학습된채로 더 이상 발전 하지 않는 문제가 일어난다.

## 3. Least Square GAN
* Mode collapsing 문제를 해결하기 위한 방법 중 하나
    * 간단하게, Loss function을 변형한 것이다.
* Objective function
    * a: 가짜 라벨, b: 실제 라벨, c: D가 속길 원하는 라벨
    * $min_D V(D)={1 \over 2} E_{x \sim P_{data}(x)} (log(D(x)-b)^2) + {1 \over 2} E_{x \sim P_z(z)} (log(D(G(z))-a)^2)$
    * $min_G V(G) = {1 \over 2} E_{x \sim P_z(z)} (log(D(G(z))-c)^2)$
* 이점
    * Decision boundary에 모여있는 real sample들에서 멀리 떨어져 있지만 real이라고 판단되는 sample들에게 panelty를 부여할 수 있다. 즉, 더 실제같은 이미지를 생성할 수 있다.
    * 앞선 sample들에 panelty가 부여되면서 Gradient vanishing 문제가 어느정도 보완이 된다.
* 논문에서 목적함수를 유도해
    * 카이제곱 분포 divergence를 최소화 하는것과 동치임을 보였다.

## 4. 그외
* F-GAN, WGAN, RGAN, RSGAN 등 굉장이 많은 GAN이 있다.
* Voice generation이나 conversion 등에 GAN이 많이 쓰이기 때문에 향후 더 공부가 필요할 것 같다. 