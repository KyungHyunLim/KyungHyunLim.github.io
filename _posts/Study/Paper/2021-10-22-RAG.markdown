---
layout: post
title:  "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"
date:   2021-10-22 11:00:22
categories: [Paper, NLP]
use_math: true
---

# 1. ì •ë³´
* ë…¼ë¬¸ëª…: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks
* í•™íšŒ: Advances in Neural Information Processing Systems
* ì €ì ì†Œì†: 
    1. Facebook AI Research
    2. University College London
    3. New York University

# 2. ë…¼ë¬¸ ë¶„ì„
## 2.1 Limitation of previous research
* Large pre-trained LM (parametric methods)
    * Factual knowledgeë¥¼ parameterì•ˆì— ì €ì¥
    * Fine-tuningì„ í†µí•´ downstream NLP taskë“¤ì—ì„œ SOTA ì„±ëŠ¥ ë‹¬ì„±
    * Limitation
        * ì§€ì‹ì— ì ‘ê·¼í•˜ê³  ë‹¤ë£¨ëŠ” ëŠ¥ë ¥ì€ ì—¬ì „íˆ ë¶€ì¡±
        * ê²°ê³¼ì— ëŒ€í•œ ê·¼ê±°ì™€ ìƒˆë¡œìš´ ì§€ì‹ì„ updateí•˜ëŠ” ê²ƒ ë˜í•œ ì•„ì§ challengeable
            * ë³€í™”í•˜ëŠ” ì§€ì‹ì„ ì‰½ê²Œ í™•ì¥, ìˆ˜ì •í•˜ê¸°ê°€ ì–´ë ¤ì›€

* Retrieval + Reader (Non-parametric methods + parametric methods)
    * í™œë°œíˆ ì—°êµ¬ë˜ê³  ìˆëŠ” ë°©ë²•ë¡ 
    * Queryì— í•´ë‹¹í•˜ëŠ” ìµœì‹  ë¬¸ì„œë¥¼ íƒìƒ‰í•´(retrieval), task ìˆ˜í–‰
    * ëŒ€í‘œì ì¸ ì—°êµ¬ë“¤: REALM, ORQA
    * Limitation
        * extractive downstream taskì— ëŒ€í•´ì„œë§Œ ì—°êµ¬ë˜ê³  ìˆìŒ

## 2.2 Propesed method overview
![](/assets/image/paper/RAG_1.png)
* Retrieval-augmented generation (RAG)
    * Generative model with non-parametric memory
* Models
    * Parametric model: pre-trained Seq2seq transformer (Reader) * BART
    * Non-parametric model: dense vector index of Wikipedia (Retrieval)
* Train
    * End-to-end í•™ìŠµ
    * Retrieval: ì¿¼ë¦¬ì— ë§ëŠ” tok-k latent documents (ë°±í„°ë‚´ì  ìœ ë„ì‚¬ë„ ê¸°ë°˜)
    * Reader: inputê³¼ retrievalì˜ ê²°ê³¼ë¥¼ í•©ì³ output ìƒì„±
* Two approach
    * ê°™ì€ ë¬¸ì„œë¥¼ í™œìš©í•´ ëª¨ë“  í† í°ì„ ìƒì„± (RAG-Sequence)
    * ê° í† í°ë§ˆë‹¤ ë‹¤ë¥¸ ë¬¸ì„œë¥¼ í™œìš©í•´ ìƒì„± (RAG-Token)
* ì•„ë¬´ seq2seq taskì— ëŒ€í•´ fine-tuning ê°€ëŠ¥

## 2.3 Notations
* ğ‘¥: input sequence
* ğ‘§: retrieve text documents
* ğ‘¦: target sequence
* $P_\eta (ğ‘§ \vert ğ‘¥)$ : retriever, query xê°€ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ êµ¬ì ˆì— ëŒ€í•œ (ìƒìœ„ Kê°œ) distributionsë¥¼ ë°˜í™˜
* $ğ‘ƒ_\theta (ğ‘¦_ğ‘– \vert ğ‘¥,ğ‘§,ğ‘¦_{1:ğ‘–âˆ’1})$ : generator, ì´ì „ i-1ê°œì˜ í† í°ê³¼ ì…ë ¥ x, ê²€ìƒ‰ëœ ë¬¸ì„œ zë¥¼ ì´ìš©í•´ í˜„ì¬ í† í°ì„ ìƒì„±

## 2.4 Models
* RAG-Sequence Model
    * ì™„ì „í•œ ë¬¸ì¥ì„ ìƒì„±í•˜ëŠ”ë° ë™ì¼í•œ retrieved document ì‚¬ìš©
    * retrieved documentë¥¼ ë‹¨ì¼ latent variableë¡œ ì·¨ê¸‰
    * Retrieval: top K documentsë¥¼ ê²°ì •
    * Generator: ê°ê° ë¬¸ì„œì—ì„œ output sequence probability ìƒì„± í›„, ìµœì¢… sequence ê²°ì • (marginalize)  
    $$P(y \vert x) = \sum_{z \in top-k(p(\cdot \vert x))} p_\eta (z \vert x) \prod_i^N ğ‘ƒ_\theta (ğ‘¦_ğ‘– |ğ‘¥,ğ‘§,ğ‘¦_(1:ğ‘–âˆ’1))   $$

* RAG-Token Model
    * ê° target tokenì„ ìƒì„±í•˜ëŠ”ë°, ë‹¤ë¥¸ latent documentë¥¼ ì‚¬ìš©
    * Generatorê°€ ë‹µì„ ìƒì„±í•  ë•Œ, ì—¬ëŸ¬ ë¬¸ì„œë¥¼ ì°¸ì¡°í•  ìˆ˜ ìˆìŒ
    * Retrieval: top K documentsë¥¼ ê²°ì •
    * Generator: ê°ê° documentë¥¼ í™œìš©í•´ ë‹¤ìŒ í† í°ì— ëŒ€í•œ distribution ìƒì„± (ìµœì¢… sequence ë¥¼ ê²°ì •í•˜ê¸° ì „ì—)  
    $$P(y \vert x) = \prod_i^N  \sum_{z \in top-k(p(\cdot \vert x))} p_\eta (z \vert x) ğ‘ƒ_\theta (ğ‘¦_ğ‘– |ğ‘¥,ğ‘§,ğ‘¦_(1:ğ‘–âˆ’1))   $$

## 2.5 Retriever: DPR
* $ğ‘ƒ_\eta (ğ‘§â”‚ğ‘¥)$ is based on Dense passage retrieval for ODQA
    $$ğ‘ƒ_\eta (ğ‘§â”‚ğ‘¥)âˆexpâ¡(ğ‘‘(ğ‘§)^ğ‘‡ ğ‘(ğ‘¥))$$
    $$ğ‘¤â„ğ‘’ğ‘Ÿğ‘’ ğ‘‘(ğ‘§)=ğµğ¸ğ‘…ğ‘‡_ğ‘‘ (ğ‘§),  ğ‘(ğ‘¥)=ğµğ¸ğ‘…ğ‘‡_ğ‘ (ğ‘¥)$$
    * ë‘ ëª¨ë¸ ëª¨ë‘ BERT-base ì‚¬ìš©
* Maximum inner product search (MIPS)ë¥¼ í™œìš©í•´ top-k ë¬¸ì„œ ê²€ìƒ‰
    * ìœ„ì— ì‹ê³¼ ê°™ì´ documentì™€ query ì„ë² ë”© ë²¡í„°ì˜ ë‚´ì  ê°’ í™œìš©
    * Sub-linear timeì— í•´ê²° ê°€ëŠ¥í•œ problem
* TriviaQA & Natural Questions ë°ì´í„°ì…‹ì˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì„ ê°€ì§„ retrieve documentsë¡œ í•™ìŠµë¨
* Non-parametric memory â€“ document index

## 2.6 Generator: BART
* $ğ‘ƒ_\theta (ğ‘¦_ğ‘– |ğ‘¥,ğ‘§,ğ‘¦_{1:ğ‘–âˆ’1})$ is based on encoder-decoder architecture
    * Seq-to-seqêµ¬ì¡°ë¥¼ ê°€ì§„ ì–´ë–¤ ëª¨ë¸ì´ë¼ë„ ì‚¬ìš©ê°€ëŠ¥ 
    * ë…¼ë¬¸ì—ì„œëŠ” BART-Large ì‚¬ìš© (400M parameters)
* Input xì™€ document z vectorë¥¼ ë‹¨ìˆœíˆ concatí•´ì„œ generate í•˜ëŠ”ë° ì‚¬ìš©
* BARTëŠ” denoising objectiveì™€ ì—¬ëŸ¬ noising functionì„ í™œìš©í•´ pretraining
    * ìœ ì‚¬í•œ parameter ìˆ˜ì˜ T5ì— ë¹„í•´, ì—¬ëŸ¬ generation taskì—ì„œ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„
* parametric memory â€“ generator parameter ğœƒ

## 2.7 Training
* Jointly train retriever + generator
    * ì–´ë–¤ ë¬¸ì„œë¥¼ retrieve í•´ì•¼ í•˜ëŠ”ì§€ ì œê³µí•˜ì§€ ì•ŠìŒ
    * ì£¼ì–´ì§„ pair $(ğ‘¥_ğ‘–, ğ‘¦_ğ‘–)$ ì— ëŒ€í•´ negative marginal log-likelihoodë¥¼ ìµœì†Œí™” í•˜ë„ë¡ í•™ìŠµ
    $$ \sum_j -log p(y_j \vert x_j)$$
    * RetrieverëŠ” ğµğ¸ğ‘…ğ‘‡_ğ‘ë§Œ fine-tuningì— ì°¸ì—¬, ğµğ¸ğ‘…ğ‘‡_ğ‘‘ëŠ” freeze
        * í›ˆë ¨ ì¤‘ì—, REALM ì²˜ëŸ¼ ğµğ¸ğ‘…ğ‘‡_ğ‘‘ë„ ì—…ë°ì´íŠ¸ í•˜ëŠ” ê²ƒì€ ë§¤ìš° ë¹„íš¨ìœ¨ì 
* Negative marginal log-likelihood  
    ![](/assets/image/paper/RAG_2.png)
    * ì •ë‹µì„ ë§ì´ ë§ì¶”ë©´ ì‘ì€ ê°’ì„ ê°–ëŠ” loss
* Optimizer: Adam

## 2.8 Decoding
* RAG-Token
    $$ ğ‘_\theta (ğ‘¦_ğ‘– |ğ‘¥,ğ‘§_ğ‘–,ğ‘¦_(1:ğ‘–âˆ’1)) = \sum_{z \in top-k(p(\cdot \vert x))} p_\eta (z_i \vert x) p_\theta (ğ‘¦_ğ‘– |ğ‘¥,ğ‘§_ğ‘–,ğ‘¦_(1:ğ‘–âˆ’1))  $$
    * Transition probability
        * the probability of moving from one state of a system into another state.
        * $ğ‘¦_(1:ğ‘–âˆ’1)$ ë¡œ ë¶€í„° $ğ‘¦_ğ‘–$ ë¥¼ ìƒì„± í•˜ê¸° ë•Œë¬¸ì—?
    * Standard beam decodeë¥¼ ì‚¬ìš©í•´ì„œ $ğ‘ƒ_\theta^â€² (ğ‘¦_ğ‘– |ğ‘¥,ğ‘¦_{1:ğ‘–âˆ’1} )$ ë¥¼ ê³„ì‚°
* RAG-Sequence
    * Single beam searchë¡œ decoding ë¶ˆê°€ëŠ¥
        * ê° document zì— ëŒ€í•´ beam search ì ìš© 
        * $ğ‘_\theta (ğ‘¦_ğ‘– |ğ‘¥,ğ‘§_ğ‘–,ğ‘¦_{1:ğ‘–âˆ’1} )$ ë¥¼ ì´ìš©í•´ ê° hypothesis scoring, hypothesizes set ğ‘Œ ìƒì„±
    * ê° yì˜ í™•ë¥  ì¶”ì •ì„ ìœ„í•´, beamì— yê°€ ë‚˜íƒ€ë‚˜ì§€ ì•Šì€ ëª¨ë“  z ë§ˆë‹¤ ì¶”ê°€ì ì¸ forward pass ì‹¤í–‰ â†’ â€œThorough Decodingâ€
    * Beam search ê²°ê³¼, yê°€ ìƒì„±ë˜ì§€ ì•Šì€ ğ‘¥, $ğ‘§_ğ‘–$ ì— ëŒ€í•´ $ğ‘_\theta (ğ‘¦|ğ‘¥,ğ‘§_ğ‘– )$ ë¥¼ 0ì— ê°€ê¹Œìš´ ê°’ì„ ê°€ì§€ë„ë¡ ì„¤ì •í•´ ì¶”ê°€ì ì¸ forward pass íšŒí”¼ â†’ â€œFast Decodingâ€

## 2.9 Data and Experimets setting
* Data
    * Non-parametric knowledge source: Wikipedia dump (December 2018)
        * ê° articleì„ 100-word chunksë¡œ ë¶„í• 
        * ì´ 21M documents ìƒì„±
        * FAISSë¡œ MIPS index ë§Œë“¤ì–´ ì‚¬ìš©
        * Top-k, (5 â‰¤ğ‘˜ â‰¤ 10)
* Tasks  
    ![](/assets/image/paper/RAG_3.png)
    * Open-domain QA: Exact match scoreë¡œ í‰ê°€
    * Abstractive QA: parametric knowledge í‰ê°€ 
        * ë‹¤ë¥¸ ë°ì´í„° ì…‹ì˜ ì§ˆë¬¸ì„ ìœ„í‚¤í”¼ë””ì•„ë§Œìœ¼ë¡œ ë‹µë³€ ë¶ˆê°€ëŠ¥
    * Jeopardy Question Generation: RAGâ€™s generation ëŠ¥ë ¥ í‰ê°€
    * Fact Verification
        * ì£¼ì–´ì§„ ë¬¸ì¥ì„ 3ê°€ì§€ë¡œ ë¶„ë¥˜
        * supports/refutes/not enough info

## 2.10 Open-domain QA & Abstractive QA
* 4ê°œì˜ ODQA ë°ì´í„° ì…‹ì—ì„œ ëª¨ë‘ RAG ê°€ ìƒˆë¡œìš´ SOTA ë‹¬ì„±
    * íŠ¹íˆ REALM/T5+SSMì™€ ë‹¤ë¥´ê²Œ RAGëŠ” salient span maskingì„ í™œìš©í•œ pre-training ê°™ì€ ë¹„ìš© ë†’ì€ ê³¼ì • ì—†ì´ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„
    * Re-ranker ë‚˜ extractive readerê°€ SOTAë¥¼ ìœ„í•´ í•„ìš”í•œ ê²ƒì´ ì•„ë‹˜ì„ ì¦ëª…
* RAGëŠ” gold passageë“¤ì„ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ìŒì—ë„ ë¶ˆêµ¬í•˜ê³ , SOTAì— ê·¼ì ‘í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì—¬ ì£¼ì—ˆìŒ  
![](/assets/image/paper/RAG_4.png)

## 2.11 Jeopardy Question Generation
* RAG-Token ëª¨ë¸ì´ ì—¬ëŸ¬ ë¬¸ì„œë¡œ ë¶€í„° ì •ë‹µì„ ìƒì„±í•˜ê¸° ë•Œë¬¸ì— ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„
    * â€œSUNâ€ ì„ ìƒì„±í•  ë•Œ, document 2ì—ì„œëŠ” í™•ë¥ ì´ ë†’ìŒ, ìœ ì‚¬í•˜ê²Œ document 1ì—ì„œëŠ” â€œA Farewell to Armsâ€ê°€ ìƒì„± ë¬ì„ ë•Œì˜ í™•ë¥ ì´ ê°€ì¥ ë†’ìŒ

    * ì²« ë²ˆì§¸ tokenì„ ìƒì„±í•œí›„ í™•ë¥ ì´ flatten ë˜ëŠ” í˜„ìƒì´ í™•ì¸ë¨
* Generatorê°€ íŠ¹ì • ë¬¸ì„œë“¤ì— ì˜ì¡´í•˜ì§€ ì•Šê³  ë¬¸ì¥ì„ ì™„ì„±í•  ìˆ˜ ìˆë‹¤ê³  suggestsí•  ìˆ˜ ìˆìŒ
* ì¦‰, ëª¨ë¸ì˜ parametric knowledgeê°€ ë¬¸ì¥ì„ ì™„ì„±í•˜ê¸°ì— ì¶©ë¶„í•˜ë‹¤ëŠ” ì˜ë¯¸
* Bartì—ê²Œ â€œThe sunâ€ì„ ì…ë ¥ìœ¼ë¡œ ì£¼ì—ˆì„ ë•Œ
    * â€œThe Sun Also Rises" is a novel by this author of "The Sun Also Risesâ€
* â€œThe Sun Also Rises" is a novel by this author of  "Aâ€ëŠ”
    * "The Sun Also Rises" is a novel by this author of "A Farewell to Arms".  
![](/assets/image/paper/RAG_5.png)

## 2.12 Additional Results
* Generation ì„±ëŠ¥
    * RAG-Sequence > RAG-Token
    * ë‘ê°œë‹¤ BARTë³´ë‹¤ëŠ” ë›°ì–´ë‚¨  
    ![](/assets/image/paper/RAG_6.png)
* Index hot-swapping
    * 2016ë…„, 2018ë…„ ë¬¸ì„œ ê¸°ë°˜ì˜ index ì¤€ë¹„
        * ì‹œê¸°ì— ë¦¬ë”ê°€ ë°”ë€ ì‚¬ë¡€ë¥¼ 82ê°€ì§€ ì¤€ë¹„
    * ì •í™•ë„=> 2016: 70%, 2018: 68%
* Fixed-BM25
    * FEVERê°€ entity centricì´ê¸° ë•Œë¬¸ì—, word overlap ë°©ì‹ì˜ BM25ê°€ ì˜ ë§ì•˜ì„ ê²ƒ  
    ![](/assets/image/paper/RAG_7.png)

## 2.13 Discussion
* Hybrid generation modelì„ ì œì•ˆ
    * Non-parametric + parametric memory
* ODQA ë¶„ì•¼ì—ì„œ SOTA ë‹¬ì„±
* Index hot-swapì„ í†µí•´, ì¬ í•™ìŠµ ì—†ì´ knowledgeë¥¼ update í•  ìˆ˜ ìˆìŒì„ ë³´ì„
* Future works
    * ë‘ ìš”ì†Œê°€ scratch ë¶€í„° pre-train í•  ë•Œ jointly ë™ì‘í•  ìˆ˜ ìˆë‹¤ë©´ ìœ ìµ
        * Denoising objectiveë¥¼ ì´ìš©
    * ë‹¤ì–‘í•œ NLP taskë“¤ì— ì ìš©í•  ìˆ˜ ìˆë„ë¡, ë‘ ìš”ì†Œë¥¼ ì–´ë–»ê²Œ íš¨ê³¼ì ìœ¼ë¡œ í•©ì¹˜ê³ , ìƒí˜¸ì‘ìš©í•˜ê²Œ í• ì§€ ìƒˆë¡œìš´ ë°©í–¥ì„± íƒêµ¬ë„ í•„ìš”

# 3. ì°¸ê³ ìë£Œ
* [Paper link](https://arxiv.org/pdf/2005.11401.pdf)
