---
layout: post
title:  "NSML 사용법 (계속 추가 중)"
date:   2021-10-19 09:30:12
categories: [ETC]
---

# Tip 1. 기본 명령

- nsml run -e(실행할 py) [asdf.py](http://asdf.py/) -d(dataset 웹에서 이름_ 우리는 dataset이이거만 쓰임) dialogue -m(memo 사용)

```python
nsml run -e main.py -d dialogue -m mymodel_test_1
```

- 제출하기

```python
nsml submit 세션 이름 checkpoint 번호
nsml submit nia2012/dialogue/268 0
```

- 세션 내 check point 확인하기

```python
nsml model ls 세션 이름
nsml model ls nia2012/dialogue/268
```

- 진행중인(실행중인) session 확인하기

```python
nsml ps
```

- 진행중인 세션 멈추기
    - 웹상에서 돌아가는 세션이 안보이는데 GPU가 사용중이라는 오류가 뜰때 ps로 확인해보고 stop 시키면 재사용 가능

```python
nsml stop 세션이름
nsml stop nia2012/dialogue/268
```

- 세션에 적은 메모 변경하기

```python
nsml memo 세션 이름 "변경할 메모"
nsml memo nia2012/dialogue/268 "change memo"
```

# Tip 2. 제출시 inference에서 오류가 난다면?
* finetuning한 모델을 submit 할 때 inference 코드에서 오류가 나는 경우,  
제출하고 싶은 check point를 다시 불러와 저장하는 .py에서 inference 코드를 수정하고 실행을 해서 만든 새로운 세션을 제출

```python
config = BartConfig()
generate_model = BartForConditionalGeneration(config=config)

bind_model(model=generate_model, types='model', parser=args)
nsml.load(checkpoint='14', session='nia2012/dialogue/171') # 제출 하고 싶은 session

if args.pause :
    nsml.paused(scope=locals())

if args.mode == 'train' :
    nsml.save(0) # check point 0에 저장
```

# Tip 3. epoch 마다 nsml checkpoint 저장하는법
* Seq2SeqTrainingArguments에 훈련 epoch 수를 1로 지정하고,   
아래에서 반복문으로 trainer.train()를 실행하면, 1epoch 씩 실행한 후에 nsml.save를 하기 때문에 에포크마다 check-point를 저장할 수 있다.

```python
training_args = Seq2SeqTrainingArguments(
          output_dir='./',
          overwrite_output_dir=True,
          num_train_epochs=1,
          per_device_train_batch_size=16,
          gradient_accumulation_steps=10,
          evaluation_strategy = 'epoch',
          save_strategy = 'epoch',
          save_total_limit=1,
          load_best_model_at_end=True,
          seed=42,
      )

for epoch in range(15):
      trainer.train()
      nsml.save(epoch)
```

# Tip 4. 학습할때, fp16 =True를 추가하자
* 학습 시간이 매우 단축 된다. 16시간 → 6시간

```python
training_args = Seq2SeqTrainingArguments(
          output_dir='./',
          overwrite_output_dir=True,
          num_train_epochs=1,
          per_device_train_batch_size=16,
          gradient_accumulation_steps=10,
          evaluation_strategy = 'epoch',
          save_strategy = 'epoch',
          save_total_limit=1,
					fp16 =True, ################## 이거 추가!!!!!
          load_best_model_at_end=True,
          seed=42,
      )
```

# Tip 5. RuntimeError: Boolean value of Tensor with more than one value is ambiguous
* generate_model.generate 할 때, input_ids에 한개씩 넣으면 안된다.  
  batch_size개씩 넣으면 돌아간다.  
  솔직히 무슨차이인지 아무리 봐도 모르겠다. 어쨋든 그렇네요....

```python
dataset = Mydataset(tokenized_encoder_inputs, len(encoder_input_test))
dataloader = DataLoader(dataset, batch_size=8)
summary = []
with torch.no_grad():
    for item in tqdm(dataloader):
        generated_ids = generate_model.generate(input_ids=item['input_ids'].to(device), max_length=50, num_beams=2)
        for ids in generated_ids:
            summary.append(tokenizer.decode(ids, skip_special_tokens=True))
```