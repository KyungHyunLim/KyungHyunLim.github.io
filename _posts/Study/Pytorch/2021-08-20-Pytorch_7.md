---
layout: post
title:  "Pytorch(7)-Hyperparameter tuning"
date:   2021-08-20 13:15:12
categories: [Pytorch, ML_AI]
use_math: true
---

## 1. 성능을 높이는 방법
1. 모델 바꾸기
2. 데이터 바꾸기 (가장... 좋은 영향)
3. Hyperparameter tuning
    * 모델 스스로 학습하지 않는 값들
        * learning rate, 모델의 크기, optimizer 등
    * 결과가 크게 좌우 될 때도 있음 (요즘은... 드뭄)
    * 마지막 0.01을 쥐어짜야 할 때 도전해볼만!

## 2. Hyperparameter Tuning
* 방법들
    * Grid search, Randomsearch (가장 기본적인 방법)
    * 최근에는 베이지안 기반 기법들이 주도 (BOHB, 2018)
* Library: Ray
    * multi-node multi processing 지원 모듈
    * ML/DL의 병렬 처리를 위한 모듈
    * 현재 분산 병렬 ML/DL의 표준 같은 존재
    * Hyperparameter Search를 위한 다양한 모듈 제공

    ```python
    # config에 search space 지정
    config = {
        "l1": tune.sample_from(lambda _:2**np.random.randint(2,9)),
        "l2": tune.sample_from(lambda _:2**np.random.randint(2,9)),
        "lr": tune.sample_from(1e-4, 1e-1)
        "batch_size": tune.choice([2, 4, 6, 8, 16])
    }

    # 학습 스케줄링 알고리즘 지정
    scheduler = ASHAScheduler(
        metric="loss", mode="min", max_t=max_num_epochs, grace_period=1, reduction_factor=2)
    )

    # 결과 출력 양식 지정
    reproter = CLIReporter(
        metric_columns=["loss", "acc", "training_iter"]
    )

    # 병렬 처리 양식으로 학습 시행
    result = tun.run(
        partial(train_cifar, data_dir=data_dir), # 데이터를 쪼개는것
        resources_pre_trial={"cpu":2, "gpu":gpus_per_trial}, # 사용할 수 있는 자원 개수
        config=config, num_samples=num_samples, #config와 sample 수
        scheduler=scheduler, # scheduler
        progress_reproter=reporter # 결과출력 양식
    )
    ```