---
layout: post
title:  "Pytorch(8)-Trouble shooting"
date:   2021-08-20 13:15:12
categories: [Pytorch, ML_AI]
use_math: true
---

## 1. OOM
### 1.1 대응하기 어려움
* 왜...를 알기 어려움
* 어디서... 도 알기 어려움
    * Error backtracking이 이상한 곳으로 빠짐
    * 메모리의 이전상황의 파악이 난해
* 주로 Iteration 안에서 발생
    * GPU clean -> RUN / Batch_size 줄이기

### 1.2 어떻게 대응할까?
* GPU util 사용하기
    * nvidia-smi GPU 상태를 보여주는 모듈
    * iter마다 메모리가 늘어나는지 확인

    ```python
    import GPUtill
    GPUtil.showUtilization()
    ```
* torch.cuda.empty_cache() 써보기
    * 사용되지 않는 GPU 위 cache 정리 -> 가용메모리 확보
    * del과는 다른것!
    * reset 대신 사용하기 좋음
* trainning loop에 tensor로 축적 되는 변수는 확인 해보기
    * Tensor로 처리된 변수는 GPU 상에 메모리 사용
    * 해당 변수는 loop안에서 computational graph를 생성하기 때문에 메모리 차지

    ```python
    total_loss = 0
    for i in range(epoch):
        optimizer.zero_grad()
        output = model(input)
        loss = Loss(output)
        loss.backward()
        optimizer.step()
        total_loss += loss 
        # loss1 + loss2 + loss3 + ... = total_loss가 되서쓸데없이 메모리를 잡아 먹는다.
    ```
    * 1- tensor의 경우 python의 기본 객체로 변환하여 처리!

    ```python
    total_loss = 0
    for i in range(epoch):
        iter_loss = torch.randn(3,4).mean()
        iter_loss.requires_grad = True
        total_loss += iter_loss.itme() # item을 붙이거나 float(iter_loss) 처럼 사용!
    ```
* del 명령어 적절히 사용
* batch 사이즈 조절하기
* torch.no_grad() 사용하기
    * inference 시점에는 gradient를 계산하지 않음
    * backward pass로 인해 사용되는 메모리에서 자유로움

## 2. 예상치 못한 에러 메세지
* CUDNN_STATUS_NOT_INIT, device-side-assert 등
    * cuda와 관련해 OOM의 일종으로 생각될 수 있다!
    * 적절한 코드 처리 필요
* colab에서 너무 큰 사이즈는 실행하지 말 것
* CNN의 대부분의 에러는 크기가 안맞아서...
    * torchsummary 등으로 크기를 맞출것!
* tensor의 float precision을 16bit으로 줄일 수도 있음!
    * 용량을 줄일 수 있지만, 그렇게 많이 사용되지는 않는다



