---
layout: post
title:  "Negative log likelihood Loss with pytorch"
date:   2021-10-29 20:00:00
categories: [Pytorch, ML_AI]
use_math: true
---

# 1. NLL loss란?
* Softmax의 출력에 $-log$ 를 적용해서 만드는 loss
* 의미?
    * 일반적으로 softmax를 취했을 때, 해당 클래스일 logit값이 높을 수록 더 큰 값을 가진다. 아래 보이는 예시와 같이 logit 값이 1.4 일 때, softmax를 취한 값이 -0.6453이고, 0.5일때 -1.1724인 것을 확인 할 수 있다.  
    ![](/assets/image/Pytorch/nll_1.PNG)  
    여기에 $-log(v)$ 를 취한 것이 NLL loss 이다. 아래 결과에서 볼수 있듯이 원하는 타겟인 0(torch.zeros로 타겟을 주었기 때문에)번 인덱스에 있는 값에 -를 취한 값이 로스가 되는 것을 볼 수 있다.  
    ![](/assets/image/Pytorch/nll_2.PNG)  
    즉, logit값이 높으면 softmax값이 커지고, 그러면 nll loss 값이 작아진다. 이말은 nll loss를 최소화 하면 logit값을 높일 수 있다는 말이 된다!  
    결과 적으로 nll loss를 최소화하는 것으로 분류 문제의 loss로 사용할 수 있다!

# 2. pytorch에서 활용하기!
* pytorch NLL loss는 아래와 같이 두개의 방법으로 호출이 가능하다. 두개의 차이는 무엇일까?
    * functional의 경우 이름 그대로 함수이다.
    * NLL loss는 클래스이다.
    * 즉, 아래 사용법과 같이 클래스는 인스턴스화를 시키고 사용할 수 있지만 함수의 경우 이러한 과정이 필요가 없다.

```python
import torch.nn.functional as F
import torch.nn as nn

tensors = torch.tensor([[1.4, 0.1, 0.2, 0.3],
                        [0.5, 0.1, 0.2, 0.3]])
targets = torch.zeros(2).long()

print(F.log_softmax(tensors))
print(nn.LogSoftmax(tensors))

print(F.nll_loss(tensors, targets))
print(nn.NLLLoss()(tensors, targets))
```