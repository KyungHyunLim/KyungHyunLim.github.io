---
layout: post
title:  "Multivariable linear/logistic regression"
date:   2021-07-26 13:01:21
categories: [ustage]
use_math: true
---

### 1. Multivariable linear regression

```
Data
Q1    Q2    Q3    Final(y)
70    80    75    75
82    71    98    83.6
76    92    91    86.3
90    95    99    94.6

H(x) = w1 * Q1 + w2 * Q2 + w3 * Q3 + b
```

```python
for e in range(1, epoch + 1)
   hyp = x_train.matmul(W) + b #H(x)

   cost = torch.mean((hyp - y_train)**2)

   optimizer = optim.zerograd() 
   # 기존 grad에 계속 더하게 됨으로 꼭 초기화를 해주어야 한다.
   cost.backward()
   optimizer.step()
```

```python
# Use nn library
import torch.nn as nn

class MultivariableLinearRegressionModel(nn.Module):
   def __init__(self):
      super().__init__()
      self.linear = nn.Linear(3, 1)

   def forward(self, x):
      return self.linear(x)

model = MultivariableLinearRegressionModel()
hyp = model.forward(x_train)
```

### 2. Data loading
 * 대용량 데이터 처러
   * 시간적 한계
   * 하드웨어적 한계
 * Minibatch Gradient Descent
   * 전체가 아니라 일부를 조금씩 학습

   ```python
   import torch.utils.data import Dataset, DataLoader

   class CustomDataset(Dataset):
      def __init__(self):
         # Omit

      # 구현해야할 메소드 1
      def _len_(self):
         return len(self.x_data)

      # 구현해야할 메소드 2
      def _getitem_(self, idx):
         x = torch.FloatTensor(self.x_data[idx])
         y = torch.FloatTensor(self.y_data[dix])
         return x, y

   dataset = CustomDataset()

   dataloader = DataLoader(dataset, batch_size=2, shuffle=True,)

   epoch = 20
   for e in range(1, epoch + 1):
      for idx, data in enumerate(dataloader):
         x_train, y_train = data
         pred = model.forward(x_train)

         #omit
   ```

### 3. Logistic regression
 * Logistic regression: binary classification
   * $ H(X) = {1 \over 1 + e^{-W^TX}} $
      * 시그모이드 함수와 유사
      * $ P(X; W) $ W가 주어졌을 때, $x$ 일 확률
   * $ cost(W) = -{1 \over m} \sum ylog(H(x)) + (1-y)(log()1-H(x)) $
 * Code
 
```python
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 import torch.optim as optim

 # omit # 

 hyp = 1 / (1 + torch.exp(-x_train.matmul(W) + b))
 
 cost = (-(y_train*torch.log(hyp)+(1-y_train)
      * torch.log(1-hpy))).mean()
 
 # same
 # cost = F.binary_cross_entoropy(hpy, y_train)

 class BinaryClassifier(nn.Module):
    def.__init__(self):
       super().__init__()
       self.layer = Sequentail(
         nn.linear = nn.Linear(8,1),
         nn.Sigmoid()
       )
   
    def forward(self, x):
       return self.layer(x)

model = BinaryClaasifier()
```

### 4. Softmax Classification

```python

```
