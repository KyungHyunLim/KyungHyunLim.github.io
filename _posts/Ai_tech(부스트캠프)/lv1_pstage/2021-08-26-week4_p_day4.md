---
layout: post
title:  "Week4(pstage) - day4"
date:   2021-08-26 20:22:30
categories: [pstage]
---

## 1. 개인학습
### 1.1 Training & Inference
* Loss 활용
    * Loss = Cost = Error
    * Loss도 nn.Module Family

    ```python
    class MSELoss(_Loss):
        def __init__()
            ...
        def forward(self, ):
            return F.mse_loss(input, target, ...)
    ```
    * loss.backward()로 grad값 업데이트
        * model의 forward ~ loss의 backward의 chain이 생긴다
* 특별한 Loss
    * Focal Loss
        * Class imbalance 문제가 있는 경우, 낮은 확률의 class에 Loss를 더 높게 부여
    * Label Smoothing Loss
        * Class target label을 Onehot 표현으로 사용하기 보다, 조금 Soft 하게 표현해서 일반화 성능을 높이는것
        * E.g. (0,1,0,0) -> (0.025, 0.9, 0.025, 0.0025)
* learning rate
    * 영리하게 움직일수록 수렴이 빠르다
    * LR scheduler
        * 점점 줄여 나감
    * StepLR
        * setp마다 lr을 줄여 나감
    * CosineAnnealingLR
        * cosin 형태처럼 LR을 급격하게 변경
    * ReduceLROnPlateau
        * 더이상 성능 향상이 없을때 LR 감소
* Metric
    * 모델 평가를 위한 지표
    * Classification
        * Acc, f1, precision, recall, ROC&AUC
    * Regression
        * MAE, MSE
    * Ranking
        * MRR, NDCG, MAP

### 1.2 Training Process
* model.train() 으로 바꿔주기
    * Dropout, BatchNorm train mode로 바꿔준다
    * eval()과 다르게 적용되어야함
* optimizer.zero_grad()
    * 이전 스텝의 그래디언트 정보를 없애기 (loss가 더해진다)
    * 그대로 쓸것인가? 초기화하고 쓸것인가? 결정 필요
* Loss
    * Chain 생성: grad_fn chain
    * -> loss.backward()
* More
    * Gradient Accumulation
        * batch 여러번 마다 update 되도록: batch size를 늘리는 효과

        ```python
        # 일정 횟수 마다 업데이트
        if i % SET_NUM == 0:
            optimizer.step()
            optimizer.zero_grad()
        ```
### 1.4 Inferenc Process
* model.eval() -> inference 모드로 전환 (dropout, batchnormalization)
* with torch.no_grad() -> 파라미터 비활성화

### 1.5 Pytorch Lightning
* keras와 유사... 간단 명료하게 구현 가능
* 하지만 공부는 pytorch로, 과정을 이해하자!

### 1.6 시각화
* Seaborn 소개 및 기초

## 2. 알고리즘 풀이
* [풀이](https://kyunghyunlim.github.io/algorithm/2021/08/26/BAEK_2667.html)

## 3. 경진대회 진행
* Data Augmentation
    * 단순 학습으로는 70% 근처가 한계인듯 보임 특히 특정 클래스의 부분이 f1-score가 떨어짐
    * 유독 적은 클래스 한번더 aug 적용
    * 학습 중에도 확률적 aug 적용

* Model
    * Efficientnet (timm 라이브러리 활용) 현재 까지 가장 성능이 좋음
    * learning rate의 중요성, 학습이 되다 말다 하다 lr을 계속 조정해보니 굉장히 학습이 잘되는 lr이 있었다. 인내하고 튜닝해보는게 답인가....?
    
* 향후 계획
    * 3개의 분류기로 나누는 것은 안해봐도 될듯하다
    * 완벽하게 모듈화 하기
    * Test 결과 확인 하는 jupyter notebook 만들기
    * 리포트 앞부분 미리 써놓기(문제정의, EDA 부분)