---
layout: post
title:  "Week4(pstage) - day3"
date:   2021-08-25 20:22:30
categories: [pstage]
---

## 1. 개인학습
### 1.1 Model with Pytorch
* Model 이란?
    * 시스템이나 객체의 표현(Informative representation)
* PyTorch
    * Reaserch to Production
    * 좋은 Open source libray
    * Low-level, Pythonic, Flexibility
    * 모든 레이어는 nn.Moudle 클래스를 따른다
* nn.Moules
    * Parameter를 저장하는 곳
    * init : 사용할 레이어 정의
    * forward: 순전파 알고리즘 정의
    * 모든 nn.Module은 child modules를 가질 수 있다
        * 내 모델을 정의하는 순간, 그 모델에 연결된 모든 module 확인 가능
        * state_dict(), parameters() 로 상세 확인 가능
        * Parameter의 변수: data, grad, requires_grad

### 1.2 Pretrained Model
* Computer Vision의 발전
    * 많은 일이 자동화 가능해 졌다(Object Detection, Self-driving, ...)
    * IMAGENET dataset: 높은 품질의 데이터 셋은 필수!!
        * 14 million images
        * 20,000 categories
    * 데이터가 있어서 발전할 수 있었다
* Pretrained Model의 배경
    * 좋은 품질, 대용량의 데이터로 학습한 모델 -> 내 목적에 맞게 다듬기
    * 쉽게 라이브러리를 활용해 가져올 수 있다
* Transfer Learning
    * CNN base 모델 구조  
    ![](/assets/images/pstage/w4_d3_1.PNG)
    * fc layer가 보통 classifier
    * Pretraining 할 때 설정했던 문제와 현재 문제와의 유사성을 고려 필요
    * Case by case
        * 학습데이터가 충분하다면?  
        ![](/assets/images/pstage/w4_d3_2.PNG)
        * 학습데이터가 충분하지 않다면?  
        ![](/assets/images/pstage/w4_d3_3.PNG)
            * 유사도가 낮으면 별 효용이 없을 수 있음
    
## 2. 알고리즘 풀이
* [풀이](https://kyunghyunlim.github.io/algorithm/2021/08/25/BAEK_2066.html)

## 3. 경진대회 진행
* Data Augmentation
    * 데이터가 없는 label들에 대해서 5배 aug 진행

* Model change
    * Vision transformer -> Efficientnet (timm 라이브러리 활용)
    * VIT의 성능이 생각보다 낮고, 모델이 커서 학습시키기가 어려웠다. OOM으로 인해 batch size를 8 밖에 줄수 없었다. 1eopch에 거의 7~10이 걸렸기 때문에 학습이 너무 오래걸렸다. GPU가 여러개였으면 하나 돌려놓고, 다른 시도를 해볼수 있었겠지만, GPU가 1대밖에 없어서 실험에 오래 걸리는 모델을 사용하지 못할 것같다. 그래서 이미지 분야에서 빠르고 성능 또한 좋다고 알려진 Efficientnet을 적용해보았다.
    
* 향후 계획
    * 18class 학습이 아닌 3종류 (마스크 착용상태, 성별, 나이) 3가지를 따로 분류하는 모델을 만들고, 예측 결과를 합쳐서 하나의 class로 만들면 모델이 추론해야하는 복잡도가 줄어들기 때문에 학습이 더 쉬워지지 않을까 예상한다.