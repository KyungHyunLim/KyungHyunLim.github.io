---
layout: post
title:  "Week1 - 5 일차"
date:   2021-08-06 21:54:56
categories: [ustage]
use_math: true
---

## 1. 개인학습
 * RNN 첫걸음
	* 시퀀스 데이터
		* 시계열 데이터-시간순서에 따라 나열된 데이터
		* 소리, 문자열, 주가 등
		* 독립동등분포(i,i,d) 가정을 잘 위배
			* 순서를 바꾸거나, 과거정보손실 $\rightarrow$ 데이터 확률분포 변화
	* 시퀀스 데이터 다루기
		*  조건부확률 이용, 베이즈 룰이용
			* $P(X_1, ..., X_t)=P(X_t \vert X_1, ..., X_{t-1})P(X_1, ..., X_{t-1})$
			* $= \prod_{s=1}^tP(X_s \vert X_{s-1},...,X_1)$
		* 과거의 모든 정보가 필요한 것은 아니다!
		* 가변적 길이: $\tau$
			* 고정길이 $\tau$ 만큼의 시퀀스만 사용하는 경우 autoregressive model ( $AR(\tau )$ )
			* 과거의 정보를 인코딩 해서 활용
				* how? -> RNN
	* RNN
		* 기본적인 형태
			* $O = HW^{(2)} + b^{(2)}$
			* $H = \sigma (XW^{(1)+b^{(1)}})$
				* $H$ : 잠재변수
				* $\sigma$ : 활성화함수
				* $W$ : 가중치행렬
				* $b$ : bias
			* 현재 시점 t와 이전 시점 $H_{t-1}$ 을 이용해 $O_t$ 를 생성
			![](/assets/image/day5_1.png)
		* 역전파
			* BTTP (Backpropagation Throuh Time)
				* O와 H로 부터 2개의 그레디언트 벡터 활용
			![](/assets/image/day5_2.png)  
			* Gradient vanishing 발생
				* 길이가 길어질 수록 곱해지는 값이 늘어남
				* 아주 작은 값 or 큰 값이 될수 있기 때문
			![](/assets/image/day5_3.png) 
			* 해결책?
				* truncated BPTT
					* 일정 부분의 그레디언트를 무시
					* t시점에는 O로 부터의 그래디언트만 받음
					![](/assets/image/day5_4.png) 
				* GRU, LSTM
					* 다른 강의에서 자세히

## 2. 피어세션
 * 모더레이터
	* CNN, RNN 강의 정리
	* 계획정리
 * 코딩테스트 스터디 
 
## 3. 선택과제 2
 * RNN backpropagation
	* $\frac{\partial \xi}{\partial W_x} = \sum_{k=0}^n {\partial \xi \over \partial S_k} {\partial S_k \over \partial W_x} = \sum_{k=0}^n {\partial \xi \over \partial S_k} x_k$
	* $\frac{\partial \xi}{\partial W_{rec}} = \sum_{k=0}^n {\partial \xi \over \partial S_k} {\partial S_k \over \partial W_{rec}} = \sum_{k=0}^n {\partial \xi \over \partial S_k} S_{k-1}$

 ```python
 # 뒤에서 부터 (가장 최근 부터)
 for k in range(X.shape[1], 0, -1):
	# x값 곱해서 평균내서 더하기 [X.shape: (100, 10)]
	wx_grad += np.sum(np.mean(grad_over_time[:,k] * X[:, k-1], axis=0))
	# 이전 S값 곱해서 평균내서 더하기 [S.shape: (100, 11)]
	wRec_grad += np.sum(np.mean(grad_over_time[:,k] * S[:, k-1], axis=0))
	# 이전 레이어 그래디언트 전파
	grad_over_time[:,k-1] = grad_over_time[:,k] * wRec

 ```