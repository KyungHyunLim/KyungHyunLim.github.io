---
layout: post
title:  "Week3 - day2"
date:   2021-08-17 20:22:30
categories: [ustage]
---

## 1. ê°œì¸í•™ìŠµ
* pytorch
	
## 2. í•„ìˆ˜ ê³¼ì œ
* Document íƒìƒ‰
    * torch.tensor() vs torch.Tensor()
    
    ```
    torch.tensor()ëŠ” í•­ìƒ ë°ì´í„°ë¥¼ ë³µì œí•©ë‹ˆë‹¤. ê·¸ë˜ì„œ data ì—†ì´ tensor ìƒì„±ì´ ë¶ˆê°€ëŠ¥ í•©ë‹ˆë‹¤. -> runtime error
    torch.Tensor()ëŠ” ê°ì²´(classì˜ instance)ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒìœ¼ë¡œ data ì—†ì´ë„ ìƒì„±ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    ```

* pytorch í™œìš©
    * ì‚¬ì¹™ì—°ì‚°
        * `+` : [torch.add](https://pytorch.org/docs/stable/generated/torch.add.html?highlight=add#torch.add)
        * `-` : [torch.sub](https://pytorch.org/docs/stable/generated/torch.sub.html#torch.sub)
        * `*` : [torch.mul](https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul)
        * `/` : [torch.div](https://pytorch.org/docs/stable/generated/torch.div.html#torch.div)
    * ì¸ë±ì‹±
* torch.gather 3ì°¨ì›ì—ì„œ ëŒ€ê°ì„  ë½‘ê¸°!

```python
import torch
import numpy as np
# TODO : ì„ì˜ì˜ í¬ê¸°ì˜ 3D tensorì—ì„œ ëŒ€ê°ì„  ìš”ì†Œ ê°€ì ¸ì™€ 2Dë¡œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“œì„¸ìš”! 
def get_diag_element_3D(A):
    # 1. ì¸ë±ìŠ¤ í…ì„œ ë§Œë“¤ê¸°
    # 1ê°œì˜ ì›ì†Œì—ì„œ ë½‘ì„ ì¸ë±ìŠ¤ [0,1,2,...]
    idx = torch.tensor([(i % A.shape[2])  for i in range(A.shape[1])])
    #print("idx: 1: ", idx, idx.shape)
    
    # 2. ì°¨ì› ë§ì¶°ì£¼ê¸° [0,1,2] -> [[0], [1], [2]]
    idx = idx.unsqueeze(1).unsqueeze(0)
    #print("idx: unsqueeze: ", idx, idx.shape)
    
    # 3. ì›ì†Œ ê°œìˆ˜ ë§Œí¼ ì¸ë±ìŠ¤ ë³µì‚¬(í™•ì¥)
    idx = idx.expand(A.shape[0], A.shape[1], 1) 
    #print("idx: expand: ", idx, idx.shape)
    
    # 4. ëŒ€ê°ì„ ê°’ë§Œ ë½‘ì•„ë‚´ê¸°
    output = torch.gather(A, 2, idx)
    #print(output.view(A.shape[0], A.shape[1])
    
    # 5. 2ì°¨ì›ìœ¼ë¡œ ë§Œë“  í›„ ë¶ˆí•„ìš”í•œ ë¶€ë¶„ slicing
    return output.view(A.shape[0], A.shape[1])[:, :A.shape[2]]
```
* LazyLinear vs Linear
    * ì „ìëŠ” ì²« forwardê°€ ë¶ˆë¦° í›„ì— initailize ëœë‹¤.
    * torch.nn.UninitializedParameter classì— ì†í•œë‹¤.
* custom_model ë§Œë“¤ê¸°
    * Tensor vs Parameter
    
    ```
    Parameterë¥¼ ì´ìš©í•´ì„œ W, bë¥¼ ë§Œë“¤ ê²½ìš°ì—ë§Œ
    output tensorì— gradientë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ì¸ grad_fnê°€ ìƒì„±ë©ë‹ˆë‹¤
    ```

    * module ë¶„ì„í•˜ê¸°

    ```python
    for name, module in model.named_modules():
        print(f"[ Name ] : {name}\n[ Module ]\n{module}")
        print("-" * 30)

    output
    ------
    [ Name ] : 
    [ Module ]
    Model(
    (ab): Layer_AB(
        (a): Function_A()
        (b): Function_B()
    )
    (cd): Layer_CD(
        (c): Function_C()
        (d): Function_D(
        (c): Function_C()
        )
    )
    )
    ------------------------------
    [ Name ] : ab
    [ Module ]
    Layer_AB(
    (a): Function_A()
    (b): Function_B()
    )
    ------------------------------
    [ Name ] : ab.a
    [ Module ]
    Function_A()
    ------------------------------
    [ Name ] : ab.b
    [ Module ]
    Function_B()
    ------------------------------
    [ Name ] : cd
    [ Module ]
    Layer_CD(
    (c): Function_C()
    (d): Function_D(
        (c): Function_C()
    )
    )
    ------------------------------
    [ Name ] : cd.c
    [ Module ]
    Function_C()
    ------------------------------
    [ Name ] : cd.d
    [ Module ]
    Function_D(
    (c): Function_C()
    )
    ------------------------------
    [ Name ] : cd.d.c
    [ Module ]
    Function_C()
    ------------------------------

    for name, child in model.named_children():
        print(f"[ Name ] : {name}\n[ Children ]\n{child}")
        print("-" * 30)
    
    [ Name ] : ab
    [ Children ]
    Layer_AB(
    (a): Function_A()
    (b): Function_B()
    )
    ------------------------------
    [ Name ] : cd
    [ Children ]
    Layer_CD(
    (c): Function_C()
    (d): Function_D(
        (c): Function_C()
    )
    )
    ------------------------------
    ```

    * parameter ë¶„ì„í•˜ê¸°

    ```python
    for name, parameter in model.named_parameters():
        print(f"[ Name ] : {name}\n[ Parameter ]\n{parameter}")
        print("-" * 30)

    output
    ------
    [ Name ] : ab.b.W1
    [ Parameter ]
    Parameter containing:
    tensor([10.], requires_grad=True)
    ------------------------------
    [ Name ] : ab.b.W2
    [ Parameter ]
    Parameter containing:
    tensor([2.], requires_grad=True)
    ------------------------------

    parameter = model.get_parameter('ab.b.W1')
    ```

    * buffer ì •ë³´ í™•ì¸

    ```python
    for name, buffer in model.named_buffers():
        print(f"[ Name ] : {name}\n[ Buffer ] : {buffer}")
        print("-" * 30)

    buffer = model.get_buffer('cd.c.duck')
    ```

    * doc string

    ```python
    def __doc__(self): 
        êµ¬í˜„!
    ```
* hook
    * í”„ë¡œê·¸ë¨ì˜ ì‹¤í–‰ ë¡œì§ì„ ë¶„ì„í•˜ê±°ë‚˜
    * í”„ë¡œê·¸ë¨ì— ì¶”ê°€ì ì¸ ê¸°ëŠ¥ì„ ì œê³µí•˜ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©
    * Forward hook

    ```python
    # TODO: ë‹µì„ x1, x2, output ìˆœì„œë¡œ listì— ì°¨ë¡€ì°¨ë¡€ ë„£ìœ¼ì„¸ìš”! 
    answer = []

    # TODO : pre_hookë¥¼ ì´ìš©í•´ì„œ x1, x2 ê°’ì„ ì•Œì•„ë‚´ answerì— ì €ì¥í•˜ì„¸ìš”
    def pre_hook(module, input):
        for v in input:
            answer.append(v)
        pass
    # TODO : hookë¥¼ ì´ìš©í•´ì„œ output ê°’ì„ ì•Œì•„ë‚´ answerì— ì €ì¥í•˜ì„¸ìš”
    def hook(module, input, output):
        answer.append(output)
        pass
    add.register_forward_pre_hook(pre_hook)
    add.register_forward_hook(hook)
    --------------------------------------------------------------------
    # TODO : hookë¥¼ ì´ìš©í•´ì„œ ì „íŒŒë˜ëŠ” output ê°’ì— 5ë¥¼ ë”í•´ë³´ì„¸ìš”!
    def hook(module, input, output):
        output = output + 5
        return output
        pass

    add.register_forward_hook(hook)
    ```

    * Backward hook

    ```python
    # TODO: ë‹µì„ x1.grad, x2.grad, output.grad ìˆœì„œë¡œ listì— ì°¨ë¡€ì°¨ë¡€ ë„£ìœ¼ì„¸ìš”! 
    answer = []

    # TODO : hookë¥¼ ì´ìš©í•´ì„œ x1.grad, x2.grad, output.grad ê°’ì„ ì•Œì•„ë‚´ answerì— ì €ì¥í•˜ì„¸ìš”
    def module_hook(module, grad_input, grad_output):
        for gi in grad_input:
            answer.append(gi)
        answer.append(grad_output[0])
        pass

    model.register_full_backward_hook(module_hook)
    ---------------------------------------------------------------
        
    # TODO : hookë¥¼ ì´ìš©í•´ì„œ Wì˜ gradient ê°’ì„ ì•Œì•„ë‚´ answerì— ì €ì¥í•˜ì„¸ìš”
    def tensor_hook(grad):
    #     print(grad)
        answer.append(grad)
        pass

    model.W.register_hook(tensor_hook)
    ```
* apply
    * apply í•¨ìˆ˜ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”(Weight Initialization)ì— ë§ì´ ì‚¬ìš©
    * applyë¥¼ í†µí•´ ì ìš©í•˜ëŠ” í•¨ìˆ˜ëŠ” ëª¨ë“  moduleë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì…ë ¥ë°›ì•„ì„œ ì²˜ë¦¬

    ```python
    # TODO : applyë¥¼ ì´ìš©í•´ ëª¨ë“  Parameter ê°’ì„ 1ë¡œ ë§Œë“¤ì–´ë³´ì„¸ìš”!
    def weight_initialization(module):
        module_name = module.__class__.__name__
        if module_name.split('_')[0] == 'Function':
            module.W = torch.nn.Parameter(torch.tensor([1.0]), requires_grad=True)
            #print(type(module))

    # ğŸ¦† applyëŠ” applyê°€ ì ìš©ëœ moduleì„ return í•´ì¤˜ìš”!
    returned_module = model.apply(weight_initialization)
    ```
    * ê¸°ì¡´ ëª¨ë¸ì— íŒŒë¼ë¯¸í„° ì¶”ê°€í•˜ê³ , forward ë°©ì‹ ë°”ê¾¸ê¸°

    ```python
    # TODO : applyë¥¼ ì´ìš©í•´ Parameter bë¥¼ ì¶”ê°€í•´ë³´ì„¸ìš”!
    def add_bias(module):
        module_name = module.__class__.__name__
        # ê° Function ëª¨ë“ˆì— bias ì¶”ê°€
        if module_name.split('_')[0] == "Function":
            module.b = Parameter(torch.rand(2, ), requires_grad=True)
            

    # TODO : applyë¥¼ ì´ìš©í•´ ì¶”ê°€ëœ bë„ ê°’ì„ 1ë¡œ ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”!
    def weight_initialization(module):
        module_name = module.__class__.__name__

        if module_name.split('_')[0] == "Function":
            module.W.data.fill_(1.)
            # Function ëª¨ë“ˆ bias ì´ˆê¸°í™”
            module.b.data.fill_(1.)


    # TODO : applyë¥¼ ì´ìš©í•´ ëª¨ë“  Functionì„ linear transformationìœ¼ë¡œ ë°”ê¿”ë³´ì„¸ìš”!
    #        X @ W + b
    def linear_transformation(module):
        module_name = module.__class__.__name__
        
        # hookì„ ì´ìš©í•´ forward í•¨ìˆ˜ ì¶œë ¥ê°’ ì„ í˜•ìœ¼ë¡œ ë³€í˜•
        def hook(moudle, input, output):
            output = (torch.matmul(input[0], module.W.T) + module.b)
            return output
        
        # ê° ëª¨ë“ˆì— hook ë“±ë¡
        if module_name == "Function_A":
            module.register_forward_hook(hook)
        elif module_name == "Function_B":
            module.register_forward_hook(hook)
        elif module_name == "Function_C":
            module.register_forward_hook(hook)
        elif module_name == "Function_D":
            module.register_forward_hook(hook)
            
    returned_module = model.apply(add_bias)
    returned_module = model.apply(weight_initialization)
    returned_module = model.apply(linear_transformation)

    ```

## 3. í”¼ì–´ì„¸ì…˜
* pytorch-basic ê°•ì˜ ì •ë¦¬
* ìš°ì„ ìˆœìœ„ í ì´ë¡  ê³µìœ 

## 4. íŠ¹ê°•
* ë¼ì´ì—‡ CTO ë‹˜