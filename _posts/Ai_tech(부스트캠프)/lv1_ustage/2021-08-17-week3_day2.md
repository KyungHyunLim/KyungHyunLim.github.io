---
layout: post
title:  "Week3 - day2"
date:   2021-08-17 20:22:30
categories: [ustage]
---

## 1. 개인학습
* pytorch
	
## 2. 필수 과제
* Document 탐색
    * torch.tensor() vs torch.Tensor()
    
    ```
    torch.tensor()는 항상 데이터를 복제합니다. 그래서 data 없이 tensor 생성이 불가능 합니다. -> runtime error
    torch.Tensor()는 객체(class의 instance)를 생성하는 것으로 data 없이도 생성이 가능합니다.
    ```

* pytorch 활용
    * 사칙연산
        * `+` : [torch.add](https://pytorch.org/docs/stable/generated/torch.add.html?highlight=add#torch.add)
        * `-` : [torch.sub](https://pytorch.org/docs/stable/generated/torch.sub.html#torch.sub)
        * `*` : [torch.mul](https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul)
        * `/` : [torch.div](https://pytorch.org/docs/stable/generated/torch.div.html#torch.div)
    * 인덱싱
* torch.gather 3차원에서 대각선 뽑기!

```python
import torch
import numpy as np
# TODO : 임의의 크기의 3D tensor에서 대각선 요소 가져와 2D로 반환하는 함수를 만드세요! 
def get_diag_element_3D(A):
    # 1. 인덱스 텐서 만들기
    # 1개의 원소에서 뽑을 인덱스 [0,1,2,...]
    idx = torch.tensor([(i % A.shape[2])  for i in range(A.shape[1])])
    #print("idx: 1: ", idx, idx.shape)
    
    # 2. 차원 맞춰주기 [0,1,2] -> [[0], [1], [2]]
    idx = idx.unsqueeze(1).unsqueeze(0)
    #print("idx: unsqueeze: ", idx, idx.shape)
    
    # 3. 원소 개수 만큼 인덱스 복사(확장)
    idx = idx.expand(A.shape[0], A.shape[1], 1) 
    #print("idx: expand: ", idx, idx.shape)
    
    # 4. 대각선값만 뽑아내기
    output = torch.gather(A, 2, idx)
    #print(output.view(A.shape[0], A.shape[1])
    
    # 5. 2차원으로 만든 후 불필요한 부분 slicing
    return output.view(A.shape[0], A.shape[1])[:, :A.shape[2]]
```
* LazyLinear vs Linear
    * 전자는 첫 forward가 불린 후에 initailize 된다.
    * torch.nn.UninitializedParameter class에 속한다.
* custom_model 만들기
    * Tensor vs Parameter
    
    ```
    Parameter를 이용해서 W, b를 만들 경우에만
    output tensor에 gradient를 계산하는 함수인 grad_fn가 생성됩니다
    ```

    * module 분석하기

    ```python
    for name, module in model.named_modules():
        print(f"[ Name ] : {name}\n[ Module ]\n{module}")
        print("-" * 30)

    output
    ------
    [ Name ] : 
    [ Module ]
    Model(
    (ab): Layer_AB(
        (a): Function_A()
        (b): Function_B()
    )
    (cd): Layer_CD(
        (c): Function_C()
        (d): Function_D(
        (c): Function_C()
        )
    )
    )
    ------------------------------
    [ Name ] : ab
    [ Module ]
    Layer_AB(
    (a): Function_A()
    (b): Function_B()
    )
    ------------------------------
    [ Name ] : ab.a
    [ Module ]
    Function_A()
    ------------------------------
    [ Name ] : ab.b
    [ Module ]
    Function_B()
    ------------------------------
    [ Name ] : cd
    [ Module ]
    Layer_CD(
    (c): Function_C()
    (d): Function_D(
        (c): Function_C()
    )
    )
    ------------------------------
    [ Name ] : cd.c
    [ Module ]
    Function_C()
    ------------------------------
    [ Name ] : cd.d
    [ Module ]
    Function_D(
    (c): Function_C()
    )
    ------------------------------
    [ Name ] : cd.d.c
    [ Module ]
    Function_C()
    ------------------------------

    for name, child in model.named_children():
        print(f"[ Name ] : {name}\n[ Children ]\n{child}")
        print("-" * 30)
    
    [ Name ] : ab
    [ Children ]
    Layer_AB(
    (a): Function_A()
    (b): Function_B()
    )
    ------------------------------
    [ Name ] : cd
    [ Children ]
    Layer_CD(
    (c): Function_C()
    (d): Function_D(
        (c): Function_C()
    )
    )
    ------------------------------
    ```

    * parameter 분석하기

    ```python
    for name, parameter in model.named_parameters():
        print(f"[ Name ] : {name}\n[ Parameter ]\n{parameter}")
        print("-" * 30)

    output
    ------
    [ Name ] : ab.b.W1
    [ Parameter ]
    Parameter containing:
    tensor([10.], requires_grad=True)
    ------------------------------
    [ Name ] : ab.b.W2
    [ Parameter ]
    Parameter containing:
    tensor([2.], requires_grad=True)
    ------------------------------

    parameter = model.get_parameter('ab.b.W1')
    ```

    * buffer 정보 확인

    ```python
    for name, buffer in model.named_buffers():
        print(f"[ Name ] : {name}\n[ Buffer ] : {buffer}")
        print("-" * 30)

    buffer = model.get_buffer('cd.c.duck')
    ```

    * doc string

    ```python
    def __doc__(self): 
        구현!
    ```
* hook
    * 프로그램의 실행 로직을 분석하거나
    * 프로그램에 추가적인 기능을 제공하고 싶을 때 사용
    * Forward hook

    ```python
    # TODO: 답을 x1, x2, output 순서로 list에 차례차례 넣으세요! 
    answer = []

    # TODO : pre_hook를 이용해서 x1, x2 값을 알아내 answer에 저장하세요
    def pre_hook(module, input):
        for v in input:
            answer.append(v)
        pass
    # TODO : hook를 이용해서 output 값을 알아내 answer에 저장하세요
    def hook(module, input, output):
        answer.append(output)
        pass
    add.register_forward_pre_hook(pre_hook)
    add.register_forward_hook(hook)
    --------------------------------------------------------------------
    # TODO : hook를 이용해서 전파되는 output 값에 5를 더해보세요!
    def hook(module, input, output):
        output = output + 5
        return output
        pass

    add.register_forward_hook(hook)
    ```

    * Backward hook

    ```python
    # TODO: 답을 x1.grad, x2.grad, output.grad 순서로 list에 차례차례 넣으세요! 
    answer = []

    # TODO : hook를 이용해서 x1.grad, x2.grad, output.grad 값을 알아내 answer에 저장하세요
    def module_hook(module, grad_input, grad_output):
        for gi in grad_input:
            answer.append(gi)
        answer.append(grad_output[0])
        pass

    model.register_full_backward_hook(module_hook)
    ---------------------------------------------------------------
        
    # TODO : hook를 이용해서 W의 gradient 값을 알아내 answer에 저장하세요
    def tensor_hook(grad):
    #     print(grad)
        answer.append(grad)
        pass

    model.W.register_hook(tensor_hook)
    ```
* apply
    * apply 함수는 일반적으로 가중치 초기화(Weight Initialization)에 많이 사용
    * apply를 통해 적용하는 함수는 모든 module들을 순차적으로 입력받아서 처리

    ```python
    # TODO : apply를 이용해 모든 Parameter 값을 1로 만들어보세요!
    def weight_initialization(module):
        module_name = module.__class__.__name__
        if module_name.split('_')[0] == 'Function':
            module.W = torch.nn.Parameter(torch.tensor([1.0]), requires_grad=True)
            #print(type(module))

    # 🦆 apply는 apply가 적용된 module을 return 해줘요!
    returned_module = model.apply(weight_initialization)
    ```
    * 기존 모델에 파라미터 추가하고, forward 방식 바꾸기

    ```python
    # TODO : apply를 이용해 Parameter b를 추가해보세요!
    def add_bias(module):
        module_name = module.__class__.__name__
        # 각 Function 모듈에 bias 추가
        if module_name.split('_')[0] == "Function":
            module.b = Parameter(torch.rand(2, ), requires_grad=True)
            

    # TODO : apply를 이용해 추가된 b도 값을 1로 초기화해주세요!
    def weight_initialization(module):
        module_name = module.__class__.__name__

        if module_name.split('_')[0] == "Function":
            module.W.data.fill_(1.)
            # Function 모듈 bias 초기화
            module.b.data.fill_(1.)


    # TODO : apply를 이용해 모든 Function을 linear transformation으로 바꿔보세요!
    #        X @ W + b
    def linear_transformation(module):
        module_name = module.__class__.__name__
        
        # hook을 이용해 forward 함수 출력값 선형으로 변형
        def hook(moudle, input, output):
            output = (torch.matmul(input[0], module.W.T) + module.b)
            return output
        
        # 각 모듈에 hook 등록
        if module_name == "Function_A":
            module.register_forward_hook(hook)
        elif module_name == "Function_B":
            module.register_forward_hook(hook)
        elif module_name == "Function_C":
            module.register_forward_hook(hook)
        elif module_name == "Function_D":
            module.register_forward_hook(hook)
            
    returned_module = model.apply(add_bias)
    returned_module = model.apply(weight_initialization)
    returned_module = model.apply(linear_transformation)

    ```

## 3. 피어세션
* pytorch-basic 강의 정리
* 우선순위 큐 이론 공유

## 4. 특강
* 라이엇 CTO 님