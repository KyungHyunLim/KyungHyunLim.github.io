---
layout: post
title:  "Precourse 수학튼튼-(2)"
date:   2021-07-24 14:42:21
categories: [ustage]
use_math: true
---

## 3. 파이썬 시각화 / 통계론
### 3.1 파이썬 시각화
 * matplotlib.pyplot
 * show로 flush
 * basic arg
    * color: 그래프 선 색
    * linestyle: 그래프 선 종류
    * label: 범례 (-legend)
    * alpha: 투명도
 * plot 종류
    * plot(x)
    * scatter(x, y)
    * bar(x, y)
    * hist(x, bins=100)
    * boxplot(data)

### 3.2 통계론
 * 모수란?
    * 통계적 모델링
        * 적절한 가정 위, 확률분포를 추정하는 것
        * But, 유한한 개수 데이터 $\rightarrow$ 근사적으로만 추정 가능
    * 모수적(parametric) 방법론
        * 데이터가 특정 확률분포를 따른다고 선험적으로(a priori) 가정
        * 가정한 분포를 결정하는 모수(parameter)를 추정
    * 비모수 방법론
        * 데이터에 따라 모델의 구조 및 모수의 개수가 가변적
        * 대부분의 기계학습 방법이 속함
        * 주의! 모수가 없는게 아님
 * 확률분포를 가정 예시
    * 데이터가 2개의 값만 갖음 $\rightarrow$ 베르누이분포
    * 데이터가 n개의 이산적인 값 $\rightarrow$ 카테고리분포
    * 데이터가 [0, 1] 사이의 값 $\rightarrow$ 베타분포
    * 데이터가 0시앙의 값 $\rightarrow$ 감마분포, 로그정규분포 등
    * 데이터가 $\mathbb{R}$ 전체에서 값을 가짐 $\rightarrow$ 정규분포, 라플라스분포 등
    * 주의! 데이터를 생성하는 원리를 먼저 고려하는 것이 원칙!
 * 표집분포(sampling distribution)
    * sampling distribution != sample distribution
    * 통계량의 확률분포, (즉, 표본평균과 표본분산의 확률분포)
    * 중심극한정리(Central limit theorem)
 * 최대가능도 추정법
    * 이론적으로 가장가능성이 높은 모수를 추정하는 방법(MLE)
    * 주어진 데이터 X에 대해 모수 $\theta$ 를 변수로 둔 함수
        * $\theta$ 를 변형시킴에 따라 값이 바뀜
        * 확률이 아님! $\theta$ 에 따라 대소 비교가 가능한 값
    * $ \hat{\theta_{MLE}}=argmax_{\theta}L(\theta ;x)=argmax_{\theta}P(x \mid \theta) $
    * log likelihood 사용
        * 로그가능도를 최적화하는 모수는 가능도를 최적화 하는 MLE가 됨
        * 데이터가 많아지면, 컴퓨터의 정확도로 계산하는 것이 불가능
        * 연산량을 $O(n^2)$ 에서 $O(n)$ 으로 줄일 수 있음
        * 음의 로그가능도를 최적화(경사하강법)
 * 딥러닝에서 최대가능도 추정법
    * 분류 문제
        * 모델의 가중치 $\theta = (W^1,...,W^L)$
        * 소프트맥스 벡터=카테고리분포의 모수 $(p_1,...,p_K)$ 를 모델링
        * 정답 레이블 $y=(y_1,...,y_K)$ (원핫 인코딩)을 이용, 소프트맥스 벡터의 로그가능도를 최적화
        * $\hat{\theta_{MLE}}=argmax_{\theta}({1 \over n} \sum_{i=1}^n \sum_{k=1}^K y_{i,k} log(MLP_\theta (x_i)_k))$
    * 확률분포의 거리
        * 쿨백-라이블러 발산(KL Divergence)
        * $KL(P \Vert Q)=\sum_{x \in \mathcal{X}} P(X)log({P(X) \over Q(X)})$ / $ \int_{\mathcal{X}} P(X)log({P(X) \over Q(X)})dx $
        * [ $KL(P \Vert Q)=$ 크로스 엔트로피 + 엔트로피 ] 로 분해가능
            * $\rightarrow -\mathbb E_{X \sim P(X)} [logQ(X)] + \mathbb E_{X \sim P(X)} [logP(X)]$
        * 정답 레이블을 P, 모델 예측을 Q라고 하면, 최대가능도 추정법은 쿨백-라이블러 발산을 최소화 하는 것과 동일
