---
layout: post
title:  "Week13(pstage) - Day3"
date:   2021-10-27 22:57:30
categories: [pstage]
---

# 1. 개인학습
* Dense retariever negative sampling 구성

# 2. MRC 대회 진행
- ✨READER 학습 용 데이터에 관한 얘기✨
- Issue 1: 베이스라인 코드 전처리를 사용하면, 총 `7,978`개의 샘플 중에 `3,328`개의 답이 [CLS] 토큰으로 처리된다.
    - Why? context를 max_length로 자르기 때문에 답이 없어질 수 있기 때문에
    - no_answer 느낌으로 일반화 성능을 올려주지 않을까?
- Issue 2: Issue 1을 활용해 데이터 Augmentation이 가능할 듯
    - 유사 passage에 여러 답을 학습 시킬 수 있기 때문에 일반화 성능을 위해 괜찮은 case 또는 noise가 될 수 있을 것 같음
        - A 질문 [SEP] 잘린부분 1 ⇒ answer
        - A 질문 [SEP] 잘린부분 2 ⇒ no_answer
        - A 질문 [SEP] 잘린부분 3 ⇒ no_answer
- Issue 3: 위 데이터를 적용하기 위해, 먼저 데이터를 통일 시키자!
    - 학습 도중 데이터를 만드는 것이아니라, 토크나이징만 하면 되도록, 아니면 토크나이징 까지 해서
        - 토크나이징 하면 길이가 달라질 수 있음 - max_length나 stride 길이 통일하기
    - Issue 1 데이터 augmentation을 적용해서 데이터를 만들어 두자!
    - 모델 후보:
        - Extractive model: Roberta, bigbird
        - Generate model: ko-bart
- Issue 4: Retriever 학습시에도 같은 passage에서 자르기 때문에, 답이 없는 부분을  굉장히 매력적인 오답(강민님 표현)으로 사용할 수 있을 듯 하다. (negative sampling)
    - 답이 없는 부분을 유사도가 낮게, 답이 있는 부분은 유사도가 높도록 학습하는 것 → 더 좋은 후보군(top-k) 구성이 가능할 듯