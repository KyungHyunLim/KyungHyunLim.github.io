---
layout: post
title:  "Week7 - Day4"
date:   2021-09-16 20:22:30
categories: [ustage]
---

# 1. 개인학습
* Feature based, Fine-tunning based
    * Feature based: 특정 task를 수행하는 network에 pre-trained language representation을 제공하는 모듈을 붙임
    * fine-tuning approach: task-specific한 parameter를 최대한 줄이고, language representation에 집중, 이후 간단한 fine-tuning을 통해 task 적용
* BERT 
    * 3가지 주요 임베딩
        * Token Embeddings
        * Segment Embeddings
        * Position Embeddings
    * Masked Language Model?? => How bi-direction?
        * BERT: Bidirectional Encoder Representations form Transformer
        * GPT는 앞단어를 보고 뒷단어를 예측하는 Decoder 구조 사용
        * BERT는 input과 mask된 token을 한번에 Encoder 구조에 넣어 사용
    * Packed sentence embedding ?
        * [SEP] token을 활용하는 것
        * E.g. I want go school [SEP] I want to meet friends
    * [CLS] - Classification embedding ?
        * [CLS] token은 문장의 시작으로 삽입되는 토큰
    * Fine-tunning process (c) / (d) 에서는 [CLS] 사용 X ?
        * Classification task에서는 사용되지만, 다른 task에서는 무시됨
        * transformer의 전체층을 다 거치고 나면 token sequence의 결합된 의미를 가지게 됨, 여기에 간단한 classifier를 붙이면 classification task를 간단하게 수행 가능
* GPT-2
    * Minimal fragmentation of words across multiple vocab tokens
    * layer가 위쪽으로 가면 갈수록, 해당하는 선형변환들이 0에 가까워 지도록 residual layer의 수로 normalization을 해준다. 즉, 위쪽으로 갈수록 layer의 영향력이 줄어들도록 구성 (?) -> 어떤 효과, 왜? 영향력이 줄어들어야 하나?
    * In paper
        * A modified initialization which accounts for the accumulation on the residual path with model depth is used. We scale the weights of residual layers at initialization by a factor of 1/√N where N is the number of residual layers.
* ALBERT
    * Sentence Order Prediction 
        * 두 연속된 segments에 대해 Negative samples. 즉, 단순하게 순서가 맞는지 아닌지를 판단하는 문제가 된다. 유사한 내용이 얼마나 되는지, 아닌지로 간단하게 판별 가능
        * 학습에 도움이 되는 것은 맞지만, 정말 그 판단이 쉬워지는 것인가?
            * 논문
                * This(SOP) forces the model to learn finer-grained distinctions about discourse-level coherence properties. As we show in Sec. 4.6, it turns out that NSP cannot solve the SOP task at all (i.e., it ends up learning the easier topic-prediction signal, and performs at random baseline level on the SOP task), while SOP can solve the NSP task to a reasonable degree, presumably based on analyzing misaligned coherence cues.
* GPT vs BERT
    * Corpus: 800M vs 2,500M
    * SEP, CLS token: fine tunning 시에만 사용 vs pre-training에도 사용
    * Batch size: 32,000 vs 128,000 (words)
    * learning rate: 5e-5 vs task-specific

# 2. 더 생각해 볼 것
* BERT의 CLS token: 왜 여기로 정보가 집중(?)될까? (Clssification을 왜 얘 가지고 만 하지?)
    * [참고자료](https://junklee.tistory.com/117)
* GPT 2: layer가 위쪽으로 가면 갈수록, 해당하는 선형변환들이 0에 가까워 지도록 residual layer의 수로 normalization을 해준다. 즉, 위쪽으로 갈수록 layer의 영향력이 줄어들도록 구성 한다는데, -> 어떤 효과, 왜? 영향력이 줄어들어야 하나?
* Sentence Order Prediction: 학습에 도움이 되는 것은 맞지만, 정말 그 판단이 쉬워지는 걸까?
