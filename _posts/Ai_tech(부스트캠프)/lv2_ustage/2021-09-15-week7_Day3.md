---
layout: post
title:  "Week7 - Day3"
date:   2021-09-15 20:22:30
categories: [ustage]
---

# 1. 개인학습
* [GPT-2, ALBERT](https://kyunghyunlim.github.io/nlp/ml_ai/2021/09/15/bert2.html)

# 2. 선택과제
* [Byte Pair Encoding](https://kyunghyunlim.github.io/nlp/ml_ai/2021/09/15/byteenc.html)

# 3. 피어세션 - 각 작성하기
- ELMO가 등장하게 된 배경은?
    - 언어모델은 양방향으로 상관관계가 있다?
    - 그동안 LSTM 기반의 NLP 모델들의 통합 느낌
- GPT와 BERT가 기존 트랜스포머와의 차이점은?
    - GPT와 BERT는 트랜스포머를 기본구조로 내장, GPT는 인코더에서도 Masked Attention을 수행. BERT는 GPT와 다른 방식들을 더 활용해 학습
- GPT-1 에 추가 된 기능은 무엇인가?
    - Special token을 활용해, 하나의 모델을 통해 다양한 task를 간단하게 수행 할 수 있도록 설계
- 기존 GPT-1과 BERT의 차이점은?
    - GPT-1과 BERT의 차이점으로는
        - Positional encoding이 학습할 수 있는 parameter인지 아닌지
        - 입력 sequence에 대해 양방향으로 영향
- BERT의 토크나이저 방법
    - 바이트페어 인코딩, WordPiece
- CLS와 SEP토큰의 역할과 출력값의 역할은?
    - SEP 토큰은 두 문장을 붙여 넣을때, 구분하기 위한 Special 토큰
    - CLS는 공부가 필요함... 어떻게 동작하는직 아직 잘 모르겠음
- BERT의 사전학습 2가지의 특징
    - MASK token 활용
    - Next Sentence Prediction
- BERT의 임베딩 방법 3가지
    - WordPiece embedding
    - Learned positional embedding
    - Segment embedding
    
# 4. 찾아 볼것
* Masked Language Model?? => How bi-direction?
* BERT 
    * Packed sentence embedding ?
    * [CLS] - Classification embedding ?
    * Finetunning process (c) / (d) 에서는 [CLS] 사용 X ?
* GPT-2
    * Minimal fragmentation of words across multiple vocab tokens
    * layer가 위쪽으로 가면 갈수록, 해당하는 선형변환들이 0에 가까워 지도록 residual layer의 수로 normalization을 해준다. 즉, 위쪽으로 갈수록 layer의 영향력이 줄어들도록 구성 (?) -> 어떤 효과, 왜? 영향력이 줄어들어야 하나?