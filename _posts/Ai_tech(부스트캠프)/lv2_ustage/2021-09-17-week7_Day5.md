---
layout: post
title:  "Week7 - Day5"
date:   2021-09-17 20:22:30
categories: [ustage]
---

# 1. 개인학습
* Hugging Face 익숙해지기
    * [Hugging Face ch1]()
    * [Hugging Face ch2]()

# 2. 멘토링 시간 정리
* BERT의 CLS token: 왜 여기로 정보가 집중(?)될까? (Clssification을 왜 얘 가지고 만 하지?)
    * CLS는 항상 같은 위치에 있기 때문에, 동일한 Segment, Position 임베딩 벡터가 더해진다. 그리고, word로서 의미가 없기 때문에 다른 토큰들과 attention 계산을 하는 과정을 전체 Sentence에 대한 정보를 넘긴다는 의미라고 볼 수 있다.
    * [참고자료](https://junklee.tistory.com/117)
* GPT 2: layer가 위쪽으로 가면 갈수록, 해당하는 선형변환들이 0에 가까워 지도록 residual layer의 수로 normalization을 해준다. 즉, 위쪽으로 갈수록 layer의 영향력이 줄어들도록 구성 한다는데, -> 어떤 효과, 왜? 영향력이 줄어들어야 하나?
    * 멘토링시간에 질문 하기
* Sentence Order Prediction: 학습에 도움이 되는 것은 맞지만, 정말 그 판단이 쉬워지는 걸까?
    * 인지과학
