---
layout: post
title:  "Week7 - Day5"
date:   2021-09-17 20:22:30
categories: [ustage]
---

# 1. 개인학습
* Hugging Face 익숙해지기
    * [Hugging Face ch1](https://kyunghyunlim.github.io/nlp/ml_ai/2021/09/17/hugging_face_1.html)
    * [Hugging Face ch2](https://kyunghyunlim.github.io/nlp/ml_ai/2021/09/17/hugging_face_2.html)

# 2. 멘토링 시간 추가 정리
* BERT의 CLS token: 왜 여기로 정보가 집중(?)될까? (Clssification을 왜 얘 가지고 만 하지?)
    * Pretraining 시 CLS 토큰이 해당 의미로 학습되었기 때문
    * 처음 부터 학습을 한다면 CLS 토큰을 원하는 위치에 두고 학습시키는 것이 가능하다. 하지만 그 위치가 일정해야한다!
* GPT 2: layer가 위쪽으로 가면 갈수록, 해당하는 선형변환들이 0에 가까워 지도록 residual layer의 수로 normalization을 해준다. 즉, 위쪽으로 갈수록 layer의 영향력이 줄어들도록 구성 한다는데, -> 어떤 효과, 왜? 영향력이 줄어들어야 하나?
    * 멘토님도 찾아봤지만 정보가 없음
    * 더 찾아보기
* Sentence Order Prediction: 학습에 도움이 되는 것은 맞지만, 정말 그 판단이 쉬워지는 걸까?
    * 수식적으로, 수학적으로 쉬워지는 것은 아니다. 실험적으로 더 context를 잘 학습한다.
