---
layout: post
title:  "훈민정음에 스며들다 - 대화요약 대회 (진행중)"
date:   2021-10-22 10:25:56
categories: [DialogueSummary, NLP]
use_math: True
---

# 0. 일정
* 사전평가: 9/27 ~ 9/30 (20팀 예선 진출)
    * 발표: 10/5
* 예선: 10/09 ~ 10/24 (20팀 예선 진출)
    * 발표: 10/27
* 본선: 11/01 ~ 11/30
    * 발표: 12/8
* 시상식 및 수상작 리뷰: 12/16

# 1. 대회 개요
## 1.1 대회 의의

## 1.2 Input and Output
* Input: 대화 시퀀스
* Output: 대화 시퀀스를 요약한 것
* [데이터 참고](https://aihub.or.kr/aidata/30714)

# 2. 대회 결과 요약
* 사전 평가 통과, 예선 진출
* 예선 진행중

# 3. 데이터
## 3.1 데이터 통계
* 사전평가
    * Train: 뉴스 및 법률 판결문
    * Test: 동일 (뉴스 비중이 높아 보임)
* 예선
    * Train: 개인 및 관계, 시사교육, 식음료, 일과직업, 주거와생활 5가지 주제에 대한 대화문
        * 약 14만건
    * Test: 비공개

## 3.2 EDA
* 사전평가
    * 결측치 존재 (개수가 적어 제거)
    * 이상한 데이터 셋 (본문보다 요약이 길어짐, 초월 요약, 매칭이 안되는 경우 다수)
        * 사전평가 기간이 3일 밖에 안되는데, 데이터를 가공하기 위한 시간이 충분하지 않아, 데이터가 학습하기 적합한 상태가 아니라고 판단
* 예선
    * AI-Hub 데이터를 기반으로 만들어진 것이라, 정형화된 형태
    * 하지만, 입력과 타겟의 문체의 괴리가 존재
        * 구어체 -> 문어체
        * 구어체에는 비문이 많고, 문법/맞춤법 등 다양한 요소가 존재
        * 타겟 요약문은 문어체로, 주어진 양식에 맞춰 사람이 작성한 듯 했음
    * 요약에 방해가 되는 글자 제거
        * E.g ㅋㅋㅋㅋ, ㅎㅎㅎㅎ, ㅜㅜㅜㅜ, 이모티콘, 일부 특수문자 등등
    * 반복되는 글자 normalize 
        * E.g. 웅웅웅! -> 웅웅!

# 4. 진행사항 기록 (시도 및 결과 정리)
* 사전평가
    * https://github.com/seujung/KoBART-summarization 활용 fine-tunning 진행
    * Epoch 0보다 Epoch 1에서 성능 향상
        * 요약문에 이메일, 전화번호 등이 남아 있음
        * 전처리는 적용하지 않는 것이 좋은가? - EDA 심화 필요?
    * Epoch 3 weight 적용
    * EDA가 중요하다
        * 학습에 사용할 수 없는 퀄리티의 데이터
            * 매칭이 안되는 요약문
            * 본문보다 긴 요약문
        * 전처리가 필요없을 수도 있다
            * 라벨을 잘보자!
            * 요약문에 특수문자, 이메일, 전화번호 등이 남아 있었음
    * 뽀로로 라이브러리를 활용해서 예선통과
        * EDA 결과 데이터 상태가 도저히 학습할만한 퀄리티가 아니었으며, fine-tuning을 시도했을 때 오히려 결과가 별로 좋지 않았다. 결론 적으로 유사한 데이터를 잘 학습한 pre-trained 모델을 사용하는 것이 최선이었다.
* 10월 5일
    * 사전 평가 통과
    * KLUE 대회에 집중
* ~10월 19일
    * custom tokenizer 학습하기
    * pre-training
    * [NSML 적응 과정](https://kyunghyunlim.github.io/etc/2021/10/19/NSML.html)
    * 허깅 페이스 BartForConditionalGeneration 모델 학습하기
    * 로컬에서 Bigbirdpegasus 모델 학습
* 10월 20일  
    * DAYCON inference 오류 수정
        * 리더보드 1등 분의 ROUGE-L 점수가 0.1이었는데, 이것도 통계적으로 분석을 하셔서 자주나오는 끝부분 (E.g 했다. 하기로 했다. 이야기하고 있다. 등등) 으로 답을 fix해서 냈을 때라고 이슈에 공유를 해주셨다. 그래서 이슈화가 되었고, 저는 입력과 요약간의 구어체-문어체라는 괴리가 있어서 잘 안나오는 구나 했는데, 대회측에서 평가코드를 검증을 했고 예측과 답이 제대로 매칭되지 않아 채점이 제대로 되지 않고 있었다고 공지해주셨다.
    * 전처리 및 special token 변경
        * 대화 요약문에, 대화가 아닌 사진, 동영상, 이모티콘 등 여러 기능들과 개인 정보가 될 수 있는 단어들이 #@사진@, #@이름@ 이런식으로 마스킹이 되어있었기 때문에, 이부분들을 전부 special token으로 취급하기로 결정했다. 이러한 entity들은 요약을 위해 어떤 의미를 갖지 않기 때문에(요약문에 나타나지 않음) 토큰화 된다면 오히려 노이즈가 될 것이라고 판단했기 때문이다. 팀원들이 이러한 마스킹들을 잘 찾아주셔서 쉽게 적용할 수 있었다.
* 10월 21일  
    * 대회 주최 측에서 평가 코드를 수정하고 배포한 이후, 우리팀도 그 양식에 맞추어 다시 제출을 해보았지만 전혀 점수가 오르지 않았다. 아니, 오히려 떨어졌다. NSML에서는 v100을 1대 밖에 사용하지 못했기 때문에, 다른 곳의 자원을 빌려서 실험을 진행해 보았을 때, 도저히 납득이 가지 않는 점수였다. 그래서 팀원들과 같이 코드에 오류가 있지 않은지 검토를 해보았지만, 논리적인 오류는 발견하기 어려웠다. 이런식으로 점수가 낮게 나오면 여전히 인덱스가 섞이는 것이 아닌가 싶었지만, 여러 방식으로 디버깅을 해본 결과 인덱스는 전혀 섞이지 않았다. 그래서 학습 도중, 요약의 결과를 print 해보자는 결론에 도달했고 그 결과는 너무 좋았다.  
    그래서 기대를 하고 제출을 했지만 여전히 점수는 그대로였다. 하지만, 출력해본 결과가 너무 좋았기 때문에 이건 분명 모델이 아니라, 코드상에 문제가 있다고 생각할 수 밖에 없었다. 한분이 잘나왔던 요약문을 NSML에 저장된 모델을 불러와 다시 돌려보자는 아이디어를 내주셨다. 그 결과는 충격적이었다. 잘했던 요약문이 완전 엉망으로 출력이 되었다. 모델 로딩을 제대로 못하고 있었던 것이다. pre-trained 된 weight를 쓰지는 못했기 때문에, 허깅페이스에 구현된 Bart의 구조만 빌리고 있었는데, save_pretrain 으로 저장을 하고, from_pretrained로 부터 불러오고 있었다. 그런데, from_pretrained에서 모델에 제대로 weight를 불러오지 못하고 있었다. 이 부분을 state_dict = torch.load(save_dir), model.load_state_dict(state_dict)이렇게 두 단계로 수정해 확실하게 불러올 수 있도록 수정을 한 후에 제출을 해보았다. 그 결과, 0.24점 대로 3등으로 올라 갈 수 있었다.
* 10월 22일
    * 시간상 제대로된 학습은 불가능하다고 판단.
    * 기존에 학습해둔 모델을 활용해, 추가 학습을 진행하거나 generate policy 변경해 제출
        * beam-search 기반
        * sampler 기반