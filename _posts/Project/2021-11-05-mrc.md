---
layout: post
title:  "KLUE - Machine Reading Comprehension (ODQA)"
date:   2021-11-05 11:05:56
categories: [KLUE, NLP]
use_math: True
---

# 1. 대회 개요
## 1.1 대회 의의
 "서울의 GDP는 세계 몇 위야?", "MRC가 뭐야?"  
우리는 궁금한 것들이 생겼을 때, 아주 당연하게 검색엔진을 활용하여 검색을 합니다. 이런 검색엔진은 최근 MRC (기계독해) 기술을 활용하며 매일 발전하고 있는데요. 본 대회에서는 우리가 당연하게 활용하던 검색엔진, 그것과 유사한 형태의 시스템을 만들어 볼 것입니다.  
Question Answering (QA)은 다양한 종류의 질문에 대해 대답하는 인공지능을 만드는 연구 분야입니다.다양한 QA 시스템 중, Open-Domain Question Answering (ODQA) 은 주어지는 지문이 따로 존재하지 않고 사전에 구축되어있는 Knowledge resource 에서 질문에 대답할 수 있는 문서를 찾는 과정이 추가되기 때문에 더 어려운 문제입니다.  
본 ODQA 대회에서 우리가 만들 모델은 two-stage로 구성되어 있습니다. 첫 단계는 질문에 관련된 문서를 찾아주는 "retriever" 단계이고, 다음으로는 관련된 문서를 읽고 적절한 답변을 찾거나 만들어주는 "reader" 단계입니다. 두 가지 단계를 각각 구성하고 그것들을 적절히 통합하게 되면, 어려운 질문을 던져도 답변을 해주는 ODQA 시스템을 여러분들 손으로 직접 만들어보게 됩니다.  
따라서, 대회는 더 정확한 답변을 내주는 모델을 만드는 팀이 좋은 성적을 거두게 됩니다.

## 1.2 Input and Output
* Retrieval
    * input: question, context
    * output: 유사도 기반 tok-k context
* Reader
    * input: question, context
    * output: question에 대해 context에서 찾은 답의 위치 (strat_index, end_index)

## 1.3 Task 정리
 Retrieval를 활용해 question에 대해 답을 찾을 수 있을 만한 context 후보를 추출하고, Reader로 해당 context들을 해석해 질문에 대한 답을 출력

## 1.4 역할
 팀장 및 Dense retrieval 설계 및 학습 파이프라인 구축

# 2. 대회 결과 요약
## 2.1 최종 Score 및 랭킹
* 최종 score 
    * public
        * Exact match: 70.420, f1-score: 79.020
        * Exact match: 70.000, f1-score: 79.800
    * private
        * Exact match:67.780, f1-score: 78.090
        * Exact match:68.330, f1-score: 78.350
* 최종 rangking: 7/19

# 3. 데이터
## 3.1 데이터 통계
* train dataset: 3,952
* validation dataset: 240
* test_data.csv: 240(public), 360(private)
* 학습을 위한 데이터는 총 3,952개 이며, 600개의 test 데이터를 통해 리더보드 순위를 갱신 (public과 private 2가지 종류의 리더보드가 운영)
* 외부 데이터 사용 불가

## 3.2 EDA
* 특수문자나 한글이 아닌 글자들이 많았지만, answer에도 똑같이 글자들이 들어있었기 때문에 지우는 것은 독이 될 것이라고 판단했다.
* 학습을 위한 데이터가 굉장히 적은 편이 었기 때문에, augmetation이 필요하다고 느꼈다.
    * 팀원분께서 미리 학습된 Question Generation 모델을 활용해 context로 부터 새로운 question들을 생성해 주셨다. 질문은 잘 생성되었지만 답이 잘 맞지 않아 다시 Question Answering 모델을 활용해 답을 달고, confidence를 주어 일정 수치 이상의 data만 활용하는 방식으로 데이터를 증강 하였다.
    * Retrieval를 학습하는 것에는 Question과 context만 있으면 되기 때문에 큰 도움이 되었다.

# 4. 시도 및 결과 정리
* Roberta-samll 기반 Dense retrieval
    * 가설: 어휘 기반의 Sprase embedding에 비해 의미기반 방법이 더 좋은 context 후보들을 구성해 줄것이다.
    * 두가지 모델을 동시에 학습해야 하기 때문에, 메모리 부담이 덜한 roberta-small 모델을 기반으로 dense retrieval를 구축하였다.
    1. Negative sampling  
     학습전 전처리를 통해 Negative 샘플들을 구축, 학습시 약 5~10개의 negative 샘플을 더해 NLL loss 기반으로 학습을 시도했다. 하지만 생각보다 학습이 잘 되지 않았다. Negative 샘플들이 유동적으로 변해야하는데 한번에 Dataloader에서 가져오도록 구현해 학습에 진전이 없었다.
    2. in-batch
     따로 전처리하는 부담을 덜기 위해, 배치내부에서 negative 샘플들을 사용해 효율적인 보다 학습이 가능했다.
    * 결과
        * Sparse embedding의 성능이 좋아 비등비등 하였다.
        * 아무래도 질문과 context가 깊은 해석이 필요한 것 보다 간단하게 답할 수 있는 것들로 구성되었기 때문인것 같다.
* BM25 + Dense retrieval 
    * 가설: 먼저 어휘 기반 유사도로 context 후보들을 선택하고 나서 의미기반으로 후보들을 최종 선택하면 더 좋은 후보들로 context가 구성될 것이다.
    * BM25로 먼저 약 300개의 context를 추출한 후, Dense retrieval로 top-100을 추리는 방법. 
    * 결과
        * 기존 Dense만 사용한 것과 유사한 성능을 보였다.

# 5. 최종 제출
* Retrieval: TF-IDF, BM25, RoBERTA-small, Sentence BERT
* Reader: RoBERTa-large, RoBERTa-large-cnn
* Ensemble
    * 지금까지 점수가 높았던 모든 모델들 Ensemble $\rightarrow$ 자연어 기반 Hard voting 방식으로 합산

# 6. 서비스 관점에서 생각되는 문제점
* top-k 문서를 추출하기 위해서 수많은 context embedding vector를 생산하는 데 많은 시간이 소요된다. 따라서 실제 서비스를 하기 위해서는 미리 임베딩 벡터를 생성해두고, 주기적으로 임베딩 벡터를 업데이트 해주는 방법을 사용해야하지 않을까 싶다.

# 7. 새롭게 해볼수 있는 시도
* Jonint learning
    * 메모리 문제로 3가지의 모델(p_encoder, q_encoder, reader)를 다루는 것이 어려웠음
* Retrieval
    * Triplet loss
        * Dense retrieval를 학습할 때, NLL loss만 사용해보았다. Triplet loss는 negative sample들을 잘 구성하면, 답이 아닌데 유사한 context를 효과적으로 멀리할 수 있기 때문에 적용해 보면 더 좋은 임베딩 벡터를 만들고, 더 나은 context 후보들을 추출 할 수 있을 것 같다.
    * Dense + Sparse
        * BM25로 유사 단어 기반 필터링을 적용하고, Dense를 통해 추출하는 작업을 해보았는데 생각보다 효과가 없었다.
        * 원래는 두 임베딩 벡터를 concat하여 새로운 벡터를 만드는 것이 목표였는데, 대회 마감시간이 다가와 시도해 볼 수 없어서 아쉽다.
            * 주의할 점은, Sparse embedding의 경우 매우 큰 차원(단어 수)를 가지고 있기 때문에 비교적 작은 dense 임베딩 벡터의 영향력이 무시 될 수 있기 때문에 Sparse  임베딩을 linear layer를 통해 한번 더 차원을 줄여줄 필요가 있다는 것이다!
            * 만약 같은 차원을 가지게 된다면, concat, summation 등 여러 방법으로 벡터를 바꾸어 볼 수 있다. 하지만 생각해보면 두 임베딩 벡터는 다른 공간(모델)로 부터 만들어 지기 때문에 각자 동일한 숫자라도 가지고 있는 정보가 다를 것 같다. 따라서 summation 보다는 concat을 하는 것이 효과가 더 좋지 않을까 싶다.
* Reader
    * Extractive + Generative
        * Extractive 방법은 start index와 end index를 추출해 주고, Generative 방법은 bos token으로 부터 답을 생성한다. 이 두방법을 섞어서 학습을 한다면 더 좋은 reader를 만들 수 있었을 것 같다.

# 8. 대회 회고
 길고 긴 대회가 막을 내렸다. MRC 대회는 약 4주간 진행이 되었는데, 첫 2주는 다른 대회를 신경쓰느라 거의 실험 및 가설을 세우지 못했었다. 다행이도 다른 대회에서 좋은 성적을 거두어 본선에 진출 할 수 있었고, 2주간 MRC에 최선을 다했다. 첫주에는 거의 꼴지에 머물렀지만, 다양한 가설들을 실험하고 검증하면서 성능을 끌어 올릴 수 있었다. 자연어처리 대회를 Relation extraction, Summarization, Machine reading comprehension 3가지 주제로 경험하면서 참 많은 것들을 배웠다. 자연어 데이터를 다룬다는 사실은 동일 했지만, 그 데이터를 가공하고 학습시킬 모델의 파이프라인은 완전히 달랐기 때문에, 매번 새로운 것을 접하는 느낌이었다. 그랬기 때문에 더 많이 배울 수 있었던 것 같다. 특히 이번 MRC 대회는 단일 모델의 성능을 보는 것이 아니라 ODQA로 retrieval와 reader 두가지의 조합이 잘 맞았었어야 하는 점도 있고, 두가지 모델을 활용하기 위한 파이프라인이 꽤 컷기 때문에 좋은 경험이 되었다. 2주밖에 시간을 투자하지 못해, 빛을 보지 못한 아이디어들이 많아서 아쉽기는 하지만 최종 순위 7(/19)등 으로 만족할 만한 등수를 기록할 수 있었다! 아직 본선에 진출한 대회가 남아있기 때문에 아쉬워할 시간은 없는 것 같다. 진행하고 있는 대회와 남은 대회들에서도 최대한 많은 것을 경험하고 배울 수 있도록 최선을 다해야겠다. 마지막 까지 불철주야 고생한 팀원들에게 너무 고맙다는 말을 전하고 싶다. CLUE팀은 뭘 해도 끝까지 잘 할 것 같은 정말 좋은 팀이다!
